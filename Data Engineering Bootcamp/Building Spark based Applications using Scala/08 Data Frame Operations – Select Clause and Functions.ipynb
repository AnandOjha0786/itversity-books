{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frame Operations – Select Clause and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this session we will be understanding how to select or project data from data frames while applying functions to extract required information\n",
    "\n",
    "* Getting Started\n",
    "* String Manipulation Functions\n",
    "* Using withColumn\n",
    "* Using selectExpr\n",
    "* Date Manipulation Functions\n",
    "* Dropping Columns\n",
    "* User Defined Functions – Simple\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Before getting into Pre-Defined functions available to process the data, let us make sure we have Data Frames to apply these functions.\n",
    "\n",
    "* We can apply toDF on Seq to create Data Frame out of a typical collection.\n",
    "* First, we will create Data Frame by name dual with column dummy and value X\n",
    "* Also, let us create Data Frame for the orders data set.\n",
    "* Most of the Data Frame APIs such as select, where, groupBy etc take column names in the form of strings or of col type.\n",
    "* Functions used in these APIs take column names in the form of col type.\n",
    "* If we have to add a constant value to the existing values in a column, we need to use lit on top of constant value.\n",
    "* We can also use $ instead of col.\n",
    "* We pass column names as strings if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dual = Seq(\"X\").toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(\"dummy\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(col(\"dummy\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select($\"dummy\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders = spark.read.json(\"/public/retail_db_json/orders\")\n",
    "orders.select(\"order_id\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below operation will look for column by name order_id2 and it will fail as our data frame does not have column with that name\n",
    "\n",
    "orders.select(\"order_id\" + 2).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//This will add 2 to each value in order_id\n",
    "orders.select(col(\"order_id\") + 2).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.select(upper($\"order_status\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.select(lower($\"order_status\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.select(initcap($\"order_status\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Manipulation Functions\n",
    "\n",
    "Let us go through some of the important functions using which we can manipulate strings. We should spend enough time to understand how to manipulate strings using available functions.\n",
    "\n",
    "* Case conversion functions – lower, upper, initcap\n",
    "* Trim functions – trim, rtrim, ltrim\n",
    "* Padding functions – lpad, rpad\n",
    "* Typecasting – we can use Hive function cast as part of selectExpr to change the data type of data to its original type (eg: date might be a string, but we can extract year part and convert into an integer)\n",
    "* getting length\n",
    "\n",
    "**Extracting Data**\n",
    "\n",
    "Let us see how we can extract data from the fields of Data Frame.\n",
    "\n",
    "* tracting data from fixed length records – substring\n",
    "* tracting data from variable length records – split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders = spark.read.json(\"/public/retail_db_json/orders\")\n",
    "orders.select(\"order_id\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.select(upper($\"order_status\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.select(lower($\"order_status\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.select(initcap($\"order_status\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// trimming unnecessary characters\n",
    "val dual = Seq(\"          X       \").toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(trim($\"dummy\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(length(trim($\"dummy\"))).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(trim($\"dummy\", \" \")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dual = Seq(7).toDF(\"dummy\")\n",
    "dual.select(lpad($\"dummy\", 2, \"0\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dual = Seq(10).toDF(\"dummy\")\n",
    "dual.select(lpad($\"dummy\", 2, \"0\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders = spark.read.json(\"/public/retail_db_json/orders\")\n",
    "orders.select(substring($\"order_date\", 1, 4)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Using split\n",
    "// Read text data using spark.read.text\n",
    "// Creates Data Frame using single field with name value\n",
    "val orders = spark.read.text(\"/public/retail_db/orders\")\n",
    "orders.select(split($\"value\", \",\")(0).alias(\"order_id\"),\n",
    "              split($\"value\", \",\")(1).alias(\"order_date\"),\n",
    "              split($\"value\", \",\")(2).alias(\"order_customer_id\"),\n",
    "              split($\"value\", \",\")(3).alias(\"order_status\")\n",
    "             ).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val orders = spark.read.json(\"/public/retail_db_json/orders\")\n",
    "orders.select(substring($\"order_date\", 1, 4)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.select(substring($\"order_date\", 1, 4).cast(\"int\").alias(\"order_year\")).show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders = spark.read.text(\"/public/retail_db/orders\")\n",
    "orders.select(split($\"value\", \",\")(0).cast(\"int\").alias(\"order_id\"),\n",
    "              split($\"value\", \",\")(1).alias(\"order_date\"),\n",
    "              split($\"value\", \",\")(2).cast(\"int\").alias(\"order_customer_id\"),\n",
    "              split($\"value\", \",\")(3).alias(\"order_status\")\n",
    "             ).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using withColumn\n",
    "\n",
    "We can use withColumn to transform data of a particular column within a Data Frame without impacting other columns.\n",
    "\n",
    "* If we want to select all the columns as well as new columns by applying some transformation logic, instead of specifying all the columns with select as well as expression – we can use withColumn.\n",
    "* All the other columns will remain untouched.\n",
    "* Syntax of withColumn – **df.withColumn(‘column_name’, EXPRESSION)**\n",
    "* In the below example we are going to discard the timestamp from order_date and then giving the column name as order_date. New Data Frame will still have 4 columns but order_date will have dates without timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders = spark.read.json(\"/public/retail_db_json/orders\")\n",
    "orders.printSchema\n",
    "orders.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ordersNew = orders.\n",
    "  withColumn(\"order_year\", substring($\"order_date\", 1, 4)).\n",
    "  withColumn(\"order_date\", split($\"order_date\", \" \")(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.withColumn(\"order_year\", expr(\"cast(substr(order_date, 1, 4) as int)\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.withColumn(\"order_year\", substring($\"order_date\", 1, 4).cast(\"int\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.\n",
    "  withColumn(\"order_status\", \n",
    "    expr(\"case when order_status in ('COMPLETE', 'CLOSED') \" + \n",
    "      \"then 'COMPLETED' else order_status end\")).\n",
    "  show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersNew.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersNew.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using selectExpr\n",
    "\n",
    "selectExpr for advanced transformations like the **case when**.\n",
    "\n",
    "* Even though functions available on Data Frames are robust, sometimes we might have to use traditional SQL style approach while applying functions.\n",
    "* We can take care of it using selectExpr. Whatever functions we pass to selectExpr should follow Hive syntax. If we use split, we need to use [] to access elements from the array generated as result for the split.\n",
    "* We also have expr which works in similar fashion while using withColumn or while using select.\n",
    "* selectExpr(“EXPRESSION”) is alias for select(expr(“EXPRESSION”))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders = spark.read.json(\"/public/retail_db_json/orders\")\n",
    "orders.printSchema\n",
    "orders.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.selectExpr(\"cast(substr(order_date, 1, 4) as int) order_year\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.\n",
    "  selectExpr(\"case when order_status in ('COMPLETE', 'CLOSED') \" + \n",
    "    \"then 'COMPLETED' else order_status end\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.\n",
    "  selectExpr(\"case when order_status in ('COMPLETE', 'CLOSED') \" + \n",
    "    \"then 'COMPLETED' else order_status end order_status\").\n",
    "  show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.\n",
    "  select(expr(\"case when order_status in ('COMPLETE', 'CLOSED') \" + \n",
    "    \"then 'COMPLETED' else order_status end order_status\")).\n",
    "  show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.\n",
    "  withColumn(\"order_status\", \n",
    "    expr(\"case when order_status in ('COMPLETE', 'CLOSED') \" + \n",
    "      \"then 'COMPLETED' else order_status end\")).\n",
    "  show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Manipulation Functions\n",
    "\n",
    "We also have to deal with dates very often. There are several functions which can be leveraged to manipulate dates.\n",
    "\n",
    "* Data arithmetic – date_add, date_sub, datediff, next_day, last_day, months_between, add_months\n",
    "* Getting the first of the month or year – trunc\n",
    "* Extracting information – dayofmonth, dayofyear, dayofweek, year, month\n",
    "* Formatting date – date_format\n",
    "* Typecasting – we can use the cast to convert data type of values of a particular column to its original type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dual = Seq(10).toDF(\"dummy\")\n",
    "\n",
    "dual.select(current_date()).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(current_timestamp()).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(current_timestamp()).first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(date_add(current_date, 3)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(date_add(current_date, -3)).first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(date_sub(current_date, 3)).first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.\n",
    "  select(datediff(lit(\"2018-07-25\"), lit(\"2018-06-15\"))).\n",
    "  show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dual = Seq(10).toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(trunc(current_date, \"mm\")).first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(trunc(current_date, \"yy\")).first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders = spark.read.json(\"/public/retail_db_json/orders\")\n",
    "orders.where(\"order_date > trunc('2014-07-01', 'mm')\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.where($\"order_date\" > trunc(lit(\"2014-07-01\"), \"mm\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(current_date()).first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(year(current_date())).first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(month(current_date())).first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual.select(dayofyear(current_date())).first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.\n",
    "  withColumn(\"order_month\", date_format($\"order_date\", \"YYYYMM\")).\n",
    "  show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.\n",
    "  withColumn(\"order_date\", date_format($\"order_date\", \"YYYYMMdd\").cast(\"int\")).\n",
    "  show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.\n",
    "  select(date_format($\"order_date\", \"YYYYMMdd\").cast(\"int\")).\n",
    "  show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns\n",
    "\n",
    "Now let us see how we can drop columns from Data Frame.\n",
    "\n",
    "* We can use **drop** to drop one or more columns from a Data Frame\n",
    "* When we use **drop**, it will create new Data Frame without the columns specified as part of drop.\n",
    "* Column Names can be passed as string type or column type. Using column type we can drop only one column at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders = spark.read.json(\"/public/retail_db_json/orders\")\n",
    "\n",
    "orders.drop($\"order_id\").show\n",
    "orders.drop(\"order_id\", \"order_status\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Functions – Simple\n",
    "\n",
    "Let us explore how we can define simple user defined functions and use them as part of Data Frame Operations as well as Spark SQL.\n",
    "\n",
    "* There are simple UDFs as well as Aggregated UDFs. For now, we will focus on simple UDFs.\n",
    "* We can create variable for function and convert into UDF, using **org.apache.spark.sql.functions.udf**. It will return object of type **org.apache.spark.sql.expressions.UserDefinedFunction**\n",
    "Once UDF is created we can use it as part of any transformation function such as select, where, filter, groupBy etc by using Data Frame Native approach.\n",
    "However, if we want to use it as part of SQL style syntax, we need to register UDF as SQL function. We can do so, by using spark.register.udf.\n",
    "Let us see a demo by creating UDF to extract year from date string and convert it to Integer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders = spark.read.json(\"/public/retail_db_json/orders\")\n",
    "orders.show\n",
    "\n",
    "val getYearAsInt: String => Int = _.split(\"-\")(0).toInt\n",
    "\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "val getYearAsIntUDF = udf(getYearAsInt)\n",
    "\n",
    "orders.select(getYearAsIntUDF($\"order_date\").alias(\"order_year\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.withColumn(\"order_year\", getYearAsIntUDF($\"order_date\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.filter(getYearAsIntUDF($\"order_date\") === 2014).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Register as temporary function to use as part of SQL Style Syntax\n",
    "spark.udf.register(\"getYearAsIntUDF\", getYearAsInt)\n",
    "orders.where(\"getYearAsIntUDF(order_date) = 2014\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.createTempView(\"orders\")\n",
    "spark.sql(\"select o.*, getYearAsIntUDF(o.order_date) order_year from orders o\").show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
