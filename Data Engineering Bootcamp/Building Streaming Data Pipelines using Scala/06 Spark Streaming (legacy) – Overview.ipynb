{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming (legacy) – Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this session we will see how we can build streaming applications using Spark Streaming (legacy). We will get data from log file populated by gen_logs and process using Spark Streaming.\n",
    "\n",
    "* Spark Streaming Overview\n",
    "* Overview of DStreams and APIs\n",
    "* Typical development process\n",
    "* Streaming Department Traffic – using socketTextStream\n",
    "* Streaming Department Traffic – using Kafka\n",
    "* Windowing Functions – Overview\n",
    "\n",
    "### Spark Streaming Overview\n",
    "Spark Streaming is used to process data in streaming fashion.\n",
    "\n",
    "* It requires web service called StreamingContext\n",
    "* Unlike SparkContext, StreamingContext runs perpetually processing data at regular intervals\n",
    "* We cannot have multiple contexts running at same time, hence if there is running SparkContext we need to stop it before we launch StreamingContext\n",
    "\n",
    "### Overview of DStreams and APIs\n",
    "Let us get overview about core components of Spark Streaming.\n",
    "\n",
    "* RDD is the main data structure in Spark\n",
    "* DStreams is nothing but series of RDDs which are created at regular intervals\n",
    "* We have transformations and operations to process data in DStreams\n",
    "* Most of the transformations are similar to RDD transformations, except that they take DStreams as input and return DStreams as output\n",
    "\n",
    "### Typical Development Process\n",
    "Let us see typical development process to build streaming applications using Spark Streaming.\n",
    "\n",
    "* Read data from source. We have plugins for different types of sources.\n",
    "    * Netcat – API as part of StreamingContext\n",
    "    * Flume\n",
    "    * Kafka\n",
    "    * and many more\n",
    "* Process data using Transformations\n",
    "* Write data to target. We can write data to different targets\n",
    "\n",
    "### Streaming Department Count – socketTextStream\n",
    "Let us get into details about building application to get streaming department count using socketTextStream.\n",
    "\n",
    "* Setup netcat\n",
    "* Develop Application\n",
    "* Build jar file\n",
    "* Ship and run jar file using spark-submit\n",
    "\n",
    "***Setting up Netcat***\n",
    "\n",
    "Netcat, which is a web service can be used to get started to read the data.\n",
    "\n",
    "* Start netcat service using host name and port number\n",
    "* We can start publishing messages to this web service\n",
    "* Sample nc command to redirect log messages from gen_logs to web service – <mark>tail_logs.sh | nc -lk 9999</mark>\n",
    "\n",
    "***Develop Application***\n",
    "\n",
    "Let us develop the program to perform streaming department count using socketTextStream (used for exploratory purposes).\n",
    "\n",
    "* Project Name:\n",
    "* Add dependencies for spark streaming to build.sbt\n",
    "* import org.apache.spark.streaming._ to import all the APIs\n",
    "* Create Spark Configuration object with master and app name\n",
    "* Pass Spark Configuration object and number of seconds to Streaming Context object. This will facilitate the Streaming Context to queue up the data for the interval equal to number of seconds and apply logic for processing.\n",
    "* Develop necessary logic to perform streaming department traffic\n",
    "* At the end use start and awaitTermination for Spark Streaming Context run perpetually\n",
    "* Build the jar file using **sbt package** command\n",
    "* Ship the jar file and run it on the lab"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// streaming-department-traffic-01-build.sbt\n",
    "\n",
    "name := \"StreamingDemo\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.11.12\"\n",
    "\n",
    "libraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.2\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-streaming\" % \"2.3.0\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# view rawstreaming-department-traffic-02-application.properties \n",
    "\n",
    "dev.execution.mode = local\n",
    "dev.data.host = localhost\n",
    "dev.data.port = 9999\n",
    "dev.output.dir = /Users/itversity/Reasearch/data/streaming_department_traffic\n",
    "\n",
    "prod.execution.mode = yarn\n",
    "prod.data.host = gw02.itversity.com\n",
    "prod.data.port = 9999\n",
    "prod.output.dir = /user/training/streaming_department_traffic"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// streaming-department-traffic-03-GetStreamingDepartmentTraffic.scala\n",
    "\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.streaming._\n",
    "\n",
    "/**\n",
    "  * Created by itversity on 02/10/18.\n",
    "  */\n",
    "object GetStreamingDepartmentTraffic {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val envConf = ConfigFactory.load.getConfig(args(0))\n",
    "    val conf = new SparkConf().\n",
    "      setAppName(\"Get Streaming Department Traffic\").\n",
    "      setMaster(envConf.getString(\"execution.mode\"))\n",
    "    val ssc = new StreamingContext(conf, Seconds(30))\n",
    "\n",
    "    val lines = ssc.socketTextStream(envConf.getString(\"data.host\"), envConf.getInt(\"data.port\"))\n",
    "    val departmentTraffic = lines.\n",
    "      filter(rec => rec.contains(\"GET /department/\")).\n",
    "      map(rec => (rec.split(\" \")(6).split(\"/\")(2), 1)).\n",
    "      reduceByKey(_ + _)\n",
    "    departmentTraffic.saveAsTextFiles(envConf.getString(\"output.dir\"))\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# streaming-department-traffic-04-spark-submit.sh \n",
    "\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode client \\\n",
    "  --class GetStreamingDepartmentTraffic \\\n",
    "  --conf spark.ui.port=12568 \\\n",
    "  --packages com.typesafe:config:1.3.2 streamingdemo_2.11-1.0.jar prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Department Count – using Kafka\n",
    "Now let us understand how to consume data from Kafka topic and perform streaming analytics on top of it at regular intervals. We can take the previous example and modify socketTextStream with Kafka related APIs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// streaming-department-traffic-kafka-01-build.sbt \n",
    "\n",
    "name := \"StreamingDemo\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.11.12\"\n",
    "\n",
    "libraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.2\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-streaming\" % \"2.3.0\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-streaming-kafka-0-10\" % \"2.3.0\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# streaming-department-traffic-kafka-02-application.properties\n",
    "\n",
    "dev.execution.mode = local\n",
    "dev.data.host = localhost\n",
    "dev.data.port = 9999\n",
    "dev.bootstrap.servers = localhost:9092\n",
    "dev.output.dir = /Users/itversity/Reasearch/data/streaming_department_traffic\n",
    "\n",
    "prod.execution.mode = yarn\n",
    "prod.data.host = gw02.itversity.com\n",
    "prod.data.port = 9999\n",
    "prod.bootstrap.servers = wn01.itversity.com:6667,wn02.itversity.com:6667\n",
    "prod.output.dir = /user/training/streaming_department_traffic"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// streaming-department-traffic-kafka-03-GetStreamingDepartmentTrafficKafka.scala\n",
    "\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.streaming.StreamingContext\n",
    "import org.apache.spark.streaming.Seconds\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "import org.apache.spark.streaming.kafka010._\n",
    "import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n",
    "import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n",
    "\n",
    "/**\n",
    "  * Created by itversity on 02/10/18.\n",
    "  */\n",
    "object GetStreamingDepartmentTrafficKafka {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val envConf = ConfigFactory.load.getConfig(args(0))\n",
    "    val conf = new SparkConf().\n",
    "      setAppName(\"Get Streaming Department Traffic\").\n",
    "      setMaster(envConf.getString(\"execution.mode\"))\n",
    "    val ssc = new StreamingContext(conf, Seconds(30))\n",
    "\n",
    "    val kafkaParams = Map[String, Object](\n",
    "      \"bootstrap.servers\" -> envConf.getString(\"bootstrap.servers\"),\n",
    "      \"key.deserializer\" -> classOf[StringDeserializer],\n",
    "      \"value.deserializer\" -> classOf[StringDeserializer],\n",
    "      \"group.id\" -> \"1\",\n",
    "      \"auto.offset.reset\" -> \"latest\",\n",
    "      \"enable.auto.commit\" -> (false: java.lang.Boolean)\n",
    "    )\n",
    "\n",
    "    val topics = Array(\"retail\")\n",
    "\n",
    "    val stream = KafkaUtils.createDirectStream[String, String](\n",
    "      ssc,\n",
    "      PreferConsistent,\n",
    "      Subscribe[String, String](topics, kafkaParams)\n",
    "    )\n",
    "\n",
    "    val lines = stream.map(_.value())\n",
    "\n",
    "    val departmentTraffic = lines.\n",
    "      filter(rec => rec.contains(\"GET /department/\")).\n",
    "      map(rec => (rec.split(\" \")(6).split(\"/\")(2), 1)).\n",
    "      reduceByKey(_ + _)\n",
    "    departmentTraffic.saveAsTextFiles(envConf.getString(\"output.dir\"))\n",
    "\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# streaming-department-traffic-kafka-04-kafka-console-producer.sh\n",
    "\n",
    "tail_logs.sh|kafka-console-producer.sh \\\n",
    "  --broker-list wn01.itversity.com:6667,wn02.itversity.com:6667 \\\n",
    "  --topic retail"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# streaming-department-traffic-kafka-05-spark-submit.sh \n",
    "\n",
    "spark-submit --class GetStreamingDepartmentTrafficKafka \\\n",
    "  --master yarn \\\n",
    "  --conf spark.ui.port=22231 \\\n",
    "  --packages com.typesafe:config:1.3.2,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.3.0 \\\n",
    "  streamingdemo_2.11-1.0.jar prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing Functions – Overview\n",
    "On top of regular transformations, Spark Streaming provide Windowing Functions.\n",
    "\n",
    "* Windowing Functions are primarily meant for use cases like moving averages\n",
    "* All the functions have ByWindow\n",
    "* These functions take window as well as sliding interval as arguments\n",
    "* If window size is 90 seconds and sliding interval is 30 seconds then we will be able to see moving 90 seconds aggregations every 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
