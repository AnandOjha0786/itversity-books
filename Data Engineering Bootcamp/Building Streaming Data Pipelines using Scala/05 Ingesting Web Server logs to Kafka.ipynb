{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting Web Server logs to Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we understood Kafka in detail let us see how we can get data from web server logs into Kafka topic. We can either use APIs as part of the application to directly produce messages to Kafka topic or use plugins like Flume, Kafka Connect etc to get data from log files generated by web applications.\n",
    "\n",
    "* Getting data into Kafka topic\n",
    "* Recap of Flume\n",
    "* Ingest data into Kafka topic from Flume\n",
    "* Ingest data into Kafka topic using Kafka Connect\n",
    "* Consuming data from Kafka topic\n",
    "\n",
    "### Getting data into Kafka topic\n",
    "Kafka topic is intermediate data structure. We can get data into Kafka topic using different approaches.\n",
    "\n",
    "* Custom applications using Producer API\n",
    "* Kafka Connect\n",
    "* Logstash\n",
    "* Flume\n",
    "* and more\n",
    "\n",
    "Producer APIs can be used as part of the web applications to push log messages to Kafka topic directly. But in some cases it might not be possible to use Producer APIs (e.g.: mission critical legacy web applications) – we need to use tools like Logstash, Flume etc.\n",
    "\n",
    "### Recap of Flume\n",
    "Flume can be used to read log messages from log files and ingest data into different types of sinks. One of them is Kafka.\n",
    "\n",
    "* Flume Agents can be used to ingest data from different sources to different sinks\n",
    "* Each flume agent is combination of source, sink and channel\n",
    "* We can configure Flume agents with different strategies – multi agent linear flow, multiplexing, replicating, consolidation etc.\n",
    "* Sample agent to replicate data from log file to HDFS and Flume agent logger.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# flume-multi-sink-hdfs.conf\n",
    "\n",
    "lm.sources = r1\n",
    "lm.sinks = k1\n",
    "lm.channels = c1\n",
    "\n",
    "lm.sources.r1.type = exec\n",
    "lm.sources.r1.command = tail -F /opt/gen_logs/logs/access.log\n",
    "\n",
    "lm.sinks.k1.type = hdfs\n",
    "lm.sinks.k1.hdfs.path = hdfs://nn01.itversity.com:8020/user/itversity/logstohdfs\n",
    "lm.sinks.k1.hdfs.filePrefix = retail\n",
    "lm.sinks.k1.hdfs.fileSuffix = .txt\n",
    "lm.sinks.k1.hdfs.rollInterval = 60\n",
    "lm.sinks.k1.hdfs.rollSize = 0\n",
    "lm.sinks.k1.hdfs.rollCount = 100\n",
    "lm.sinks.k1.hdfs.fileType = DataStream\n",
    "\n",
    "lm.channels.c1.type = memory\n",
    "lm.channels.c1.capacity = 1000\n",
    "lm.channels.c1.transactionCapacity = 100\n",
    "\n",
    "lm.sources.r1.channels = c1\n",
    "lm.sinks.k1.channel = c1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest data into Kafka topic from Flume\n",
    "Here are the steps to get data into Kafka topic from Flume. We will use multiplexing and replicate data into both HDFS as well as Kafka topic.\n",
    "\n",
    "* Define source type – exec or spooldir\n",
    "* Define sinks – HDFS or Kafka\n",
    "* Define channels – Memory or File\n",
    "* Run Flume agent and make sure data gets into Kafka\n",
    "* Use kafka-console-consumer.sh to confirm data is coming into Kafka."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# flume-multi-sink-kafka.conf \n",
    "\n",
    "lm.sources = r1\n",
    "lm.sinks = k1 k2\n",
    "lm.channels = c1 c2\n",
    "\n",
    "lm.sources.r1.type = exec\n",
    "lm.sources.r1.command = tail -F /opt/gen_logs/logs/access.log\n",
    "\n",
    "lm.sinks.k1.type = hdfs\n",
    "lm.sinks.k1.hdfs.path = hdfs://nn01.itversity.com:8020/user/itversity/logstohdfs_%Y-%m-%d\n",
    "lm.sinks.k1.hdfs.filePrefix = retail\n",
    "lm.sinks.k1.hdfs.fileSuffix = .txt\n",
    "lm.sinks.k1.hdfs.rollInterval = 60\n",
    "lm.sinks.k1.hdfs.rollSize = 0\n",
    "lm.sinks.k1.hdfs.rollCount = 100\n",
    "lm.sinks.k1.hdfs.fileType = DataStream\n",
    "lm.sinks.k1.hdfs.useLocalTimeStamp = true\n",
    "\n",
    "lm.sinks.k2.type = org.apache.flume.sink.kafka.KafkaSink\n",
    "lm.sinks.k2.kafka.bootstrap.servers = wn01.itversity.com:6667,wn02.itversity.com:6667\n",
    "lm.sinks.k2.topic = logstokafka\n",
    "\n",
    "lm.channels.c1.type = memory\n",
    "lm.channels.c1.capacity = 1000\n",
    "lm.channels.c1.transactionCapacity = 100\n",
    "\n",
    "lm.channels.c2.type = memory\n",
    "lm.channels.c2.capacity = 1000\n",
    "lm.channels.c2.transactionCapacity = 100\n",
    "\n",
    "lm.sources.r1.channels = c1 c2\n",
    "lm.sinks.k1.channel = c1\n",
    "lm.sinks.k2.channel = c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest data into Kafka topic using Kafka Connect\n",
    "We can also use Kafka connect to get data from log files into Kafka topic.\n",
    "\n",
    "* We can either use standalone or distributed mode for Kafka connect worker\n",
    "* Examples are available under ```$KAFKA_CONF_DIR```\n",
    "* Kafka support bunch of sources and sinks as part of connect\n",
    "* To get data from log files we need to use file as source\n",
    "* Example for file source is available under ```$KAFKA_CONF_DIR/connect-file-source.properties```\n",
    "* We can start the worker by passing 2 arguments – one worker mode and other source type.\n",
    "* We can validate by consuming data using kafka-console-consumer.sh\n",
    "\n",
    "### Consuming data from Kafka topic\n",
    "\n",
    "Once data is in Kafka topic, we can consume data using different approaches. Approach is determined based up on the requirements.\n",
    "\n",
    "* Kafka connect\n",
    "* Kafka Streams or Kafka SQL\n",
    "* Flume\n",
    "* Spark Streaming or Storm or Flink\n",
    "* Kafka connect and Flume are used for replicating data as is with no or minimum row level transformations.\n",
    "* If we have to apply transformations such as aggregations we need to use Kafka Streams or Kafka SQL or any other streaming analytics tools such as Spark Streaming\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
