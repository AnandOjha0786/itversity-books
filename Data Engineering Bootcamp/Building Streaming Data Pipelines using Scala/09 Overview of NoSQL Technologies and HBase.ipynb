{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of NoSQL Technologies and HBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this topic we will go through some key concepts of NoSQL and getting started with HBase. We will start with use cases in which NoSQL Databases are used, and then jump into HBase to understand more about NoSQL as well as HBase.\n",
    "\n",
    "* Overview of NoSQL Databases\n",
    "* NoSQL Concepts/Features\n",
    "* Setup HBase Locally\n",
    "* Understanding HBase\n",
    "* Setup Project\n",
    "* Put and Get examples using Scala\n",
    "* Develop GettingStarted using Scala\n",
    "* Develop NYSELoad using Scala\n",
    "\n",
    "### Overview of NoSQL Databases\n",
    "Let us go through the rationale behind NoSQL Databases. As part of the evolution of Databases, one of the key milestones is RDBMS. They are extensively used for applications where transactions are involved.\n",
    "\n",
    "But over a period of time, we have other types of systems where transactions are not that important – eg: recommendation engine, endorsements, messengers etc. For these type of applications using RDBMS is counterproductive. NoSQL provide us alternative databases which are not only cheaper but also scalable.\n",
    "\n",
    "***Use Cases***\n",
    "\n",
    "Let us see some use cases.\n",
    "\n",
    "* Facebook Messenger\n",
    "* Majority of LinkedIn Website components such as job recommendations, endorsements etc.\n",
    "* Streaming Executive Dashboards which are time series in nature.\n",
    "\n",
    "***List of NoSQL Databases***\n",
    "\n",
    "Let us go through some of the popular NoSQL databases available for us. Even though concepts are same, syntax and semantics are different.\n",
    "\n",
    "* HBase – comes as part of Hadoop eco system with most of the distributions such as Cloudera, Hortonworks, MapR etc.\n",
    "* Cassandra – the most popular NoSQL database which can work with Big Data technologies such as Hadoop, Spark etc.\n",
    "* MongoDB – simple, easy to use NoSQL database with rich querying capabilities and works seamlessly with Big Data technologies such as Hadoop, Spark etc.\n",
    "* DynamoDB – popular NoSQL database with in amazon eco system\n",
    "* MapR DB – MapR version of HBase.\n",
    "\n",
    "***Job Roles***\n",
    "\n",
    "Let us see the job roles who use NoSQL Databases.\n",
    "\n",
    "* Data Engineers – NoSQL is a popular choice in building streaming pipelines.\n",
    "* Application Developers – Building applications with minimum transaction features.\n",
    "\n",
    "***Learning Process***\n",
    "\n",
    "Let us see the typical learning process for exploring NoSQL Databases.\n",
    "\n",
    "* Setting up a development environment with the programming language or framework of your choice.\n",
    "* Understand CLI based tools or IDEs to interact with them.\n",
    "* Basic CRUD Operations\n",
    "* Querying Features\n",
    "* Developing Applications based on requirements.\n",
    "Ability to build applications using NoSQL is key.\n",
    "\n",
    "### NoSQL Concepts/Features\n",
    "All NoSQL databases comes with these capabilities\n",
    "\n",
    "* Key and Value – each row will have a key and value. Value typically contain attributes and respective values. It is very close to XML or JSON.\n",
    "* Indexed – Data will be typically sorted and indexed based on row key.\n",
    "* Partitioned – Data will be partitioned by row key column. It also known as sharded/sharding.\n",
    "* Replication – There will be multiple copies of data\n",
    "* Commit log – For restore and recovery of the data\n",
    "* CAP algorithm – Consistency, Accessibility and Partition Tolerance\n",
    "* Minor and major compaction – periodic merging of files with in each partition so that we will not end up having too many small files\n",
    "* Tombstones – soft delete of data\n",
    "* Vacuum Cleaning – hard delete of data\n",
    "* Consistency Level – Commit point. As we have multiple copies of data consistency level determines whether data is considered to be committed when one copy is updated or all the copies are updated. It is determined depending up on the criticality of data and desired performance for the application.\n",
    "\n",
    "<q>*While indexing and partitioning serves the purpose of scalability in terms of performance, replication serves the purpose of reliability of database. Even though we do not cover these terms extensively, it is good to understand all these terms in detail.*</q>\n",
    "\n",
    "### Setup HBase Locally\n",
    "Let us see the instructions to setup HBase locally.\n",
    "* HBase is dependent on Zookeeper\n",
    "* Zookeeper comes as part of the HBase itself\n",
    "* If you already have zookeeper you need to make sure the port and zookeeper directory are changed.\n",
    "* Download, untar and unzip tar ball downloaded (using tar xzf)\n",
    "* Copy untarred and uncompressed directoy under /opt\n",
    "* Create soft link /opt/hbase to manage upgrades in future\n",
    "* Update /opt/hbase/conf/hbase-site.xml"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<!--hbase-site.xml-->\n",
    "\n",
    "<?xml version=\"1.0\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "/**\n",
    " *\n",
    " * Licensed to the Apache Software Foundation (ASF) under one\n",
    " * or more contributor license agreements.  See the NOTICE file\n",
    " * distributed with this work for additional information\n",
    " * regarding copyright ownership.  The ASF licenses this file\n",
    " * to you under the Apache License, Version 2.0 (the\n",
    " * \"License\"); you may not use this file except in compliance\n",
    " * with the License.  You may obtain a copy of the License at\n",
    " *\n",
    " *     http://www.apache.org/licenses/LICENSE-2.0\n",
    " *\n",
    " * Unless required by applicable law or agreed to in writing, software\n",
    " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    " * See the License for the specific language governing permissions and\n",
    " * limitations under the License.\n",
    " */\n",
    "-->\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>hbase.zookeeper.quorum</name>\n",
    "    <value>localhost</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>hbase.zookeeper.property.clientPort</name>\n",
    "    <value>3181</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>hbase.zookeeper.property.dataDir</name>\n",
    "    <value>/opt/hbase/zookeeper/data</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>hbase.rootdir</name>\n",
    "    <value>/opt/hbase/data</value>\n",
    "  </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Update PATH to reflect /opt/hbase/bin\n",
    "* Start HBase, it will take care of starting Zookeeper as well (start-hbase.sh)\n",
    "* Validate by running some commands – list tables, create table, insert record and scan table."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# validate-hbase.sh\n",
    "\n",
    "list\n",
    "create 'demo', 'cf'\n",
    "put 'demo', 1, 'cf:c1', 'v1'\n",
    "scan 'demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using lab for the demonstrations.\n",
    "\n",
    "### Understanding HBase\n",
    "Let us go through the basics of HBase.\n",
    "\n",
    "* Review Multi-Node Cluster\n",
    "* Understanding HBase Shell\n",
    "* CRUD Operations\n",
    "* Schema in HBase\n",
    "\n",
    "***Review Multi-Node Cluster***\n",
    "\n",
    "Let us review details with respect to the Multi-Node Cluster.\n",
    "* Zookeeper – 3 Nodes\n",
    "* Masters – 3 Nodes\n",
    "* Region Servers – On all worker nodes\n",
    "* Data will be permanently stored in HDFS. However, data will be first copied to memory in region servers and will be flushed into HDFS at regular intervals.\n",
    "\n",
    "***Understanding hbase shell***\n",
    "\n",
    "Let us understand more about hbase shell by going through some of the commands\n",
    "* On the gateway node of the hbase cluster <mark>run hbase shell</mark>\n",
    "* <mark>help</mark> the command provides a list of commands in different categories\n",
    "* Namespace – a group of tables (similar to schema or database)\n",
    "    * create – <mark>create_namespace 'training'</mark>\n",
    "    * list – <mark>list_namespace 'training'</mark>\n",
    "    * list tables – <mark>list_namespace_tables 'training'</mark>\n",
    "* Table – a group of rows which have keys and values\n",
    "    * While creating the table we need to specify a table name and at least one column family\n",
    "    * Column family will have cells. A cell is nothing but, a name and value pair\n",
    "    * e.g.: <mark>create 'training:hbasedemo', 'cf1'<mark>\n",
    "    * list – <mark>list 'training:.*'<mark>\n",
    "    * describe – <mark>describe 'training:hbasedemo'<mark>\n",
    "    * truncate – <mark>truncate 'training:hbasedemo'<mark>\n",
    "    * Dropping is 2 step process – disable and drop\n",
    "    * disable – <mark>disable 'training:hbasedemo'<mark>\n",
    "    * drop – <mark>drop 'training:hbasedemo'<mark>\n",
    "* Inserting/updating data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "put 'training:hbasedemo', 1, 'cf1:column1', 'value1'\n",
    "put 'training:hbasedemo', 1, 'cf1:column2', 'value2'\n",
    "put 'training:hbasedemo', 4, 'cf1:column1', 'value1'\n",
    "put 'training:hbasedemo', 4, 'cf1:column3', 'value3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRUD Operations\n",
    "Let us explore details about CRUD.\n",
    "As part of this course, we will be covering CRUD operations on HBase or MapR-DB database.\n",
    "\n",
    "* CRUD stands for\n",
    "    * Create (insert)\n",
    "    * Read\n",
    "    * Update\n",
    "    * Delete\n",
    "* Here are some of the points to remember with respect to CRUD operations\n",
    "    * All the databases support all operations. But when it comes to DML (CUD), performance varies depending upon the consistency level.\n",
    "    * All databases support basic operations\n",
    "        * Selecting all the data\n",
    "        * Selecting a range of columns\n",
    "        * Retrieve row value by passing key\n",
    "        * Apply filter on row values\n",
    "    * But databases such as MongoDB also have rich aggregation framework\n",
    "\n",
    "***Schema in HBase/MapR-DB***\n",
    "\n",
    "Let us go through a quick overview of HBase/MapR-DB schema.\n",
    "\n",
    "* Recap of Database Operations\n",
    "    * A table contains a column family (a group of columns)\n",
    "    * We do not specify columns while creating the table\n",
    "    * Data is inserted/updated using put\n",
    "    * Data can be read using the scan or get. We need to pass the row key to get.\n",
    "    * Data can be deleted using delete\n",
    "    * With each put, we only insert/update one row with column within a column family\n",
    "    * Combination of row key and one column name and value are also known as cell\n",
    "    * Data is automatically sorted and partitioned on the row key\n",
    "* We need to design our row key based on the way data is stored internally.\n",
    "* There are 2 types of schemas in HBase – Thick Schema and Thin Schema\n",
    "* In RDBMS typically we will have Normalized Data Model where the relationships are established and enforced. In NoSQL, * we typically do not enforce relationships at the database layer and schemas need not be normalized.\n",
    "* Within each row key, all the cells (column name and value) are sorted based on the key of a cell (column name)\n",
    "* They support several filters (partial scan and filters on top of cells as part of get)\n",
    "\n",
    "### Setup Project\n",
    "Here are the steps involved to setup the project\n",
    "\n",
    "* Make sure necessary tables is created (training:hbasedemo, nyse:stock_data, nyse:stock_data_wide)\n",
    "* Create new project HBaseDemo using IntelliJ\n",
    "    * Choose scala 2.11\n",
    "    * Choose sbt 0.13.x\n",
    "* Make sure JDK is chosen\n",
    "* Update build.sbt. See below\n",
    "* Define application properties\n",
    "\n",
    "***Dependencies (build.sbt)***\n",
    "\n",
    "HBase applications are dependent upon Hadoop and hence we need to add dependencies related to Hadoop as well as HBase.\n",
    "\n",
    "* Add type safe config dependency so that we can externalize properties\n",
    "* Add hadoop dependencies\n",
    "* Add hbase dependencies\n",
    "* Add assembly plugin, so that we can build a fat jar\n",
    "    * Create **assembly.sbt** file in the **project** directory.\n",
    "    * Add this line of code – <mark>addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.9\")</mark>\n",
    "* Define a merge strategy. It is required to build fat jar so that we can deploy and run on other environments.\n",
    "* Replace build.sbt with below lines of code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// HBaseDemo-build.sbt\n",
    "\n",
    "name := \"HBaseDemo\"\n",
    "\n",
    "version := \"0.1\"\n",
    "\n",
    "scalaVersion := \"2.11.12\"\n",
    "\n",
    "libraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.2\"\n",
    "libraryDependencies += \"org.apache.hadoop\" % \"hadoop-common\" % \"2.7.0\"\n",
    "libraryDependencies += \"org.apache.hbase\" % \"hbase-client\" % \"1.1.2\"\n",
    "libraryDependencies += \"org.apache.hbase\" % \"hbase-common\" % \"1.1.2\"\n",
    "\n",
    "assemblyMergeStrategy in assembly := {\n",
    "  case m if m.toLowerCase.endsWith(\"manifest.mf\") => MergeStrategy.discard\n",
    "  case m if m.startsWith(\"META-INF\") => MergeStrategy.discard\n",
    "  case PathList(\"javax\", \"servlet\", xs@_*) => MergeStrategy.first\n",
    "  case PathList(\"org\", \"apache\", xs@_*) => MergeStrategy.first\n",
    "  case \"about.html\" => MergeStrategy.rename\n",
    "  case \"reference.conf\" => MergeStrategy.concat\n",
    "  case _ => MergeStrategy.first\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Externalize Properties***\n",
    "\n",
    "We need to make sure that application can be run in different environments. It is very important to understand how to externalize properties and pass the information at run time.\n",
    "\n",
    "* Make sure build.sbt have dependency related to type safe config\n",
    "* Create new directory under src/main by name resources\n",
    "* Add file called application.properties and add below entries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# HBaseDemo-application.properties\n",
    "\n",
    "dev.zookeeper.quorum = localhost\n",
    "dev.zookeeper.port = 2181\n",
    "\n",
    "prod.zookeeper.quorum = nn01.itversity.com,nn02.itversity.com,rm01.itversity.com\n",
    "prod.zookeeper.port = 2181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put and Get Examples (using sbt console)\n",
    "As we have added necessary dependencies we can use sbt console to launch scala with all dependencies made available to scala to see examples using Scala REPL or CLI.\n",
    "\n",
    "* Launch Scala REPL using sbt console\n",
    "* Import all the necessary classes or objects or functions\n",
    "* Create HBase connection object using zookeeper quorum and port\n",
    "* Create table object by using appropriate table name (make sure table is pre created using hbase <mark>shell create 'training:hbasedemo'</mark>\n",
    "* To insert a new cell\n",
    "    * Create put object\n",
    "    * Add necessary columns\n",
    "    * Add or update record using put function on table object\n",
    "    * Validate by running <mark>scan 'training:hbasedemo'</mark>\n",
    "* To get one row by using key\n",
    "    * Create get object\n",
    "    * Get row using table.get(key)\n",
    "    * Read individual cell and pass it to functions such as Bytes.toString to typecast data to an original format"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// HBaseDemo-01-Put.scala\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.hbase.client._\n",
    "import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}\n",
    "import org.apache.hadoop.hbase.util.Bytes\n",
    "\n",
    "val hbaseConf = HBaseConfiguration.create()\n",
    "hbaseConf.set(\"hbase.zookeeper.quorum\", \"localhost\")\n",
    "hbaseConf.set(\"hbase.zookeeper.property.clientPort\", \"2181\")\n",
    "\n",
    "val connection = ConnectionFactory.createConnection(hbaseConf)\n",
    "\n",
    "val table = connection.getTable(TableName.valueOf(\"training:hbasedemo\"))\n",
    "\n",
    "val row = new Put(Bytes.toBytes(\"4\"))\n",
    "row.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"column2\"), Bytes.toBytes(\"value2\"))\n",
    "row.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"column4\"), Bytes.toBytes(\"value4\"))\n",
    "\n",
    "table.put(row)\n",
    "\n",
    "table.close\n",
    "connection.close"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// HBaseDemo-02-Get.scala \n",
    "\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.hbase.client._\n",
    "import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}\n",
    "import org.apache.hadoop.hbase.util.Bytes\n",
    "\n",
    "val hbaseConf = HBaseConfiguration.create()\n",
    "hbaseConf.set(\"hbase.zookeeper.quorum\", \"localhost\")\n",
    "hbaseConf.set(\"hbase.zookeeper.property.clientPort\", \"2181\")\n",
    "\n",
    "val connection = ConnectionFactory.createConnection(hbaseConf)\n",
    "\n",
    "val table = connection.getTable(TableName.valueOf(\"training:hbasedemo\"))\n",
    "\n",
    "val key = new Get(Bytes.toBytes(\"4\"))\n",
    "val row = table.get(key)\n",
    "val v1 = row.getValue(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"column2\"))\n",
    "Bytes.toString(v1)\n",
    "\n",
    "val v2 = row.getValue(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"column4\"))\n",
    "Bytes.toString(v2)\n",
    "\n",
    "table.close\n",
    "connection.close"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// HBaseDemo-03-Scan.scala\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.hbase.client._\n",
    "import org.apache.hadoop.hbase.{CellUtil, HBaseConfiguration, TableName}\n",
    "import org.apache.hadoop.hbase.util.Bytes\n",
    "\n",
    "val hbaseConf = HBaseConfiguration.create()\n",
    "hbaseConf.set(\"hbase.zookeeper.quorum\", \"localhost\")\n",
    "hbaseConf.set(\"hbase.zookeeper.property.clientPort\", \"2181\")\n",
    "\n",
    "val connection = ConnectionFactory.createConnection(hbaseConf)\n",
    "\n",
    "val table = connection.getTable(TableName.valueOf(\"training:hbasedemo\"))\n",
    "\n",
    "val scan = new Scan()\n",
    "\n",
    "val scanner = table.getScanner(scan)\n",
    "var result = scanner.next()\n",
    "\n",
    "while (result != null) {\n",
    "  for(cell <- result.rawCells()) {\n",
    "    println(\"row key:\" + Bytes.toString(CellUtil.cloneRow(cell)) +\n",
    "      \":column family:\" + Bytes.toString(CellUtil.cloneFamily(cell)) +\n",
    "      \":column name:\" + Bytes.toString(CellUtil.cloneQualifier(cell)) +\n",
    "      \":value:\" + Bytes.toString(CellUtil.cloneValue(cell)))\n",
    "  }\n",
    "  result = scanner.next()\n",
    "}\n",
    "\n",
    "table.close\n",
    "connection.close\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop GettingStarted Program\n",
    "Now let us develop program called GettingStarted, validate using IDE, build and run on cluster.\n",
    "\n",
    "***Create GettingStarted using IDE***\n",
    "\n",
    "We will create object file using IDE to develop the logic.\n",
    "\n",
    "* Create scala program by choosing Scala Class and then type Object\n",
    "* Make sure program is named as GettingStarted\n",
    "* First we need to import necessary APIs\n",
    "* Develop necessary logic\n",
    "    * Get the properties from application.properties\n",
    "    * Load zookeeper.quorum and zookeeper.port and create HBase connection\n",
    "    * Perform necessary operations to demonstrate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// HBaseDemo-GettingStarted.scala\n",
    "\n",
    "import com.typesafe.config.{Config, ConfigFactory}\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.hbase.client._\n",
    "import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}\n",
    "import org.apache.hadoop.hbase.util.Bytes\n",
    "\n",
    "/**\n",
    "  * Created by itversity on 18/05/18.\n",
    "  */\n",
    "object GettingStarted {\n",
    "  def getHbaseConnection(conf: Config, env: String): Connection ={\n",
    "    //Create Hbase Configuration Object\n",
    "    val hbaseConfig: Configuration = HBaseConfiguration.create()\n",
    "    hbaseConfig.set(\"hbase.zookeeper.quorum\", conf.getString(\"zookeeper.quorum\"))\n",
    "    hbaseConfig.set(\"hbase.zookeeper.property.clientPort\", conf.getString(\"zookeeper.port\"))\n",
    "    if(env != \"dev\") {\n",
    "      hbaseConfig.set(\"zookeeper.znode.parent\", \"/hbase-unsecure\")\n",
    "      hbaseConfig.set(\"hbase.cluster.distributed\", \"true\")\n",
    "    }\n",
    "    val connection = ConnectionFactory.createConnection(hbaseConfig)\n",
    "    connection\n",
    "  }\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val env = args(0)\n",
    "    val conf = ConfigFactory.load.getConfig(env)\n",
    "    val connection = getHbaseConnection(conf, env)\n",
    "    val table = connection.getTable(TableName.valueOf(args(1)))\n",
    "\n",
    "    val put = new Put(Bytes.toBytes(\"2\"))\n",
    "    put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"column1\"), Bytes.toBytes(\"value1\"))\n",
    "    put.addColumn(Bytes.toBytes(\"cf1\"), Bytes.toBytes(\"column2\"), Bytes.toBytes(\"value2\"))\n",
    "\n",
    "    table.put(put)\n",
    "\n",
    "    table.close\n",
    "    connection.close()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Program takes 2 arguments, environment to load respective properties and HBase table name\n",
    "* We can go to Run -> Edit Configurations and pass arguments\n",
    "* If dev is passed it will try to connect to HBase installed locally otherwise it will connect to cluster specified in prod.zookeeper.quorum\n",
    "\n",
    "***Build, Deploy and Run***\n",
    "\n",
    "As development and validation is done, now let us see how we can build and deploy on the cluster.\n",
    "\n",
    "* Right click on the project and copy path\n",
    "* Go to terminal and run cd command with the path copied\n",
    "* Make sure assembly plugin is added\n",
    "* Run <mark>sbt assembly</mark>\n",
    "* It will generate fat jar. Fat jar is nothing but our application along with all the dependency jars integrated\n",
    "* Copy to the server where you want to deploy\n",
    "* Run using java -jar command – <mark>java -jar HBaseDemo-assembly-0.1.jar prod training:hbasedemo</mark>\n",
    "\n",
    "### Develop NYSELoad using Scala\n",
    "As part of this program we will see how we can read data from a file and load data into nyse:stock_data using Scala as programming language using HBase APIs.\n",
    "\n",
    "* Read data from file (we will only process one file at a time)\n",
    "* Create HBase Connection\n",
    "* Create table object for nyse:stock_data\n",
    "* For each record build put object and load into HBase table using table object (for performance reasons we can add multiple rows together)\n",
    "* We will also see how to add main class as part of assembly, reassemble the fat jar and run it on the cluster (use sbt assembly)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// HBaseDemo-01-NYSELoad-thin-build.sbt \n",
    "\n",
    "name := \"HBaseDemo\"\n",
    "\n",
    "version := \"0.1\"\n",
    "\n",
    "scalaVersion := \"2.11.12\"\n",
    "\n",
    "libraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.2\"\n",
    "libraryDependencies += \"org.apache.hadoop\" % \"hadoop-common\" % \"2.7.0\"\n",
    "libraryDependencies += \"org.apache.hbase\" % \"hbase-client\" % \"1.1.2\"\n",
    "libraryDependencies += \"org.apache.hbase\" % \"hbase-common\" % \"1.1.2\"\n",
    "\n",
    "assemblyMergeStrategy in assembly := {\n",
    "  case m if m.toLowerCase.endsWith(\"manifest.mf\") => MergeStrategy.discard\n",
    "  case m if m.startsWith(\"META-INF\") => MergeStrategy.discard\n",
    "  case PathList(\"javax\", \"servlet\", xs@_*) => MergeStrategy.first\n",
    "  case PathList(\"org\", \"apache\", xs@_*) => MergeStrategy.first\n",
    "  case \"about.html\" => MergeStrategy.rename\n",
    "  case \"reference.conf\" => MergeStrategy.concat\n",
    "  case _ => MergeStrategy.first\n",
    "}\n",
    "\n",
    "mainClass in assembly := Some(\"NYSELoad\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// HBaseDemo-02-thin-NYSELoad.scala\n",
    "\n",
    "import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}\n",
    "import org.apache.hadoop.hbase.client.{Connection, ConnectionFactory, Put, Table}\n",
    "import org.apache.hadoop.hbase.util.Bytes\n",
    "import com.typesafe.config.{Config, ConfigFactory}\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "\n",
    "import scala.io.Source\n",
    "\n",
    "/**\n",
    "  * Created by itversity on 27/08/18.\n",
    "  */\n",
    "object NYSELoad {\n",
    "\n",
    "  def getHbaseConnection(conf: Config, env: String): Connection ={\n",
    "    //Create Hbase Configuration Object\n",
    "    val hbaseConfig: Configuration = HBaseConfiguration.create()\n",
    "    hbaseConfig.set(\"hbase.zookeeper.quorum\",\n",
    "      conf.getString(\"zookeeper.quorum\"))\n",
    "    hbaseConfig.set(\"hbase.zookeeper.property.clientPort\",\n",
    "      conf.getString(\"zookeeper.port\"))\n",
    "    if(env != \"dev\") {\n",
    "      hbaseConfig.set(\"zookeeper.znode.parent\", \"/hbase-unsecure\")\n",
    "      hbaseConfig.set(\"hbase.cluster.distributed\", \"true\")\n",
    "    }\n",
    "    val connection = ConnectionFactory.createConnection(hbaseConfig)\n",
    "    connection\n",
    "  }\n",
    "\n",
    "  def buildPutList(table: Table, nyseRecord: String, schemaType: String) = {\n",
    "    val nyseAttributes = nyseRecord.split(\",\")\n",
    "    val put = schemaType match {\n",
    "      case \"thin\" => {\n",
    "        val put = new Put(Bytes.toBytes(\n",
    "          nyseAttributes(1) + \":\" +\n",
    "            nyseAttributes(0)))\n",
    "        // Key\n",
    "        put.addColumn(Bytes.toBytes(\"sd\"),\n",
    "          Bytes.toBytes(\"op\"),\n",
    "          Bytes.toBytes(nyseAttributes(2)))\n",
    "        put.addColumn(Bytes.toBytes(\"sd\"),\n",
    "          Bytes.toBytes(\"hp\"),\n",
    "          Bytes.toBytes(nyseAttributes(3)))\n",
    "        put.addColumn(Bytes.toBytes(\"sd\"),\n",
    "          Bytes.toBytes(\"lp\"),\n",
    "          Bytes.toBytes(nyseAttributes(4)))\n",
    "        put.addColumn(Bytes.toBytes(\"sd\"),\n",
    "          Bytes.toBytes(\"cp\"),\n",
    "          Bytes.toBytes(nyseAttributes(5)))\n",
    "        put.addColumn(Bytes.toBytes(\"sd\"),\n",
    "          Bytes.toBytes(\"v\"),\n",
    "          Bytes.toBytes(nyseAttributes(6)))\n",
    "        put\n",
    "      }\n",
    "    }\n",
    "    put\n",
    "  }\n",
    "\n",
    "  def readFilesAndLoad(table: Table, nysePath: String, schemaType: String): Unit = {\n",
    "    val nyseData = Source.fromFile(nysePath).getLines()\n",
    "    nyseData.foreach(record => {\n",
    "      val row = buildPutList(table, record, schemaType)\n",
    "      table.put(row)\n",
    "    })\n",
    "  }\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val env = args(0)\n",
    "    val conf = ConfigFactory.load.getConfig(env)\n",
    "    val connection = getHbaseConnection(conf, env)\n",
    "    val table = connection.\n",
    "      getTable(TableName.valueOf(args(2)))\n",
    "    val schemaType = args(3)\n",
    "\n",
    "    readFilesAndLoad(table, args(1), schemaType)\n",
    "\n",
    "    table.close\n",
    "    connection.close\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# HBaseDemo-03-thin-NYSELoad.sh\n",
    "\n",
    "# Make sure hbase table is created\n",
    "# create 'nyse:stock_data_thin', 'sd'\n",
    "\n",
    "java -jar \\\n",
    "  target/scala-2.11/HBaseDemo-assembly-0.1.jar \\\n",
    "  prod /data/nyse/NYSE_2016.txt nyse:stock_data_thin thin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
