{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Streaming Technologies and Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this topic, we will see the overview of technologies used in building Streaming data pipelines. Also, we will have a deeper look into Spark Structured Streaming by developing solution for a simple problem.\n",
    "\n",
    "* Overview of Streaming Technologies\n",
    "* Spark Structured Streaming – Overview\n",
    "* Setup Project\n",
    "* Develop logic using REPL\n",
    "* Development Life Cycle – IDE\n",
    "* Output Modes and Sinks\n",
    "* Windowing and Handling Late Data\n",
    "\n",
    "### Overview of Streaming Technologies\n",
    "Let us go through the details about Streaming Technologies.\n",
    "\n",
    "* Ingestion\n",
    "* Real-Time Processing\n",
    "* Databases\n",
    "* Visualization\n",
    "* Frameworks\n",
    "\n",
    "***Ingestion***\n",
    "\n",
    "There are many technologies which are used in ingesting data in real time.\n",
    "\n",
    "* Logstash\n",
    "    * Can read data from different sources\n",
    "    * Can apply simple row level transformations such as converting date formats, masking some of the attribute values in each message etc.\n",
    "    * Can work with other technologies in building streaming pipelines such as Kafka\n",
    "* Flume\n",
    "    * Runs as agent\n",
    "    * Each agent have a source, channel, and sink\n",
    "    * Supports many sources and sinks\n",
    "    * Can work with other technologies in building streaming pipelines such as Kafka\n",
    "    * Can push data to technologies like Storm, Flink, Spark Streaming etc to run real-time streaming analytics.\n",
    "* Kafka connect and Kafka topic\n",
    "    * Kafka topic is false tolerant and highly reliable intermediate data streaming mechanism\n",
    "    * Kafka connect is to read data from different sources and push messages to Kafka topic and also consume messages from Kafka topic and push to supported targets.\n",
    "    * Kafka connect and the topic will facilitate us to get data from different types of sources to different types of sinks.\n",
    "    * Can push data to technologies like Kafka Streams, Storm, Flink, Spark Streaming etc to run real-time streaming analytics.\n",
    "* Kinesis firehose and Kinesis data streams\n",
    "    * Kinesis is AWS Service which is very similar to Kafka\n",
    "    * Kinesis Firehose is similar to Kafka connect and Kinesis data streams is similar to the topic\n",
    "    * No need for dedicated cluster and will only be charged for the usage.\n",
    "* and more\n",
    "\n",
    "***Real-Time processing***\n",
    "\n",
    "As the data come through tools like logstash, flume, kafka etc we might want to perform standard transformations such as data cleansing, standardization, lookups, joins, aggregations, sorting, ranking etc. While some of the data ingestion tools are capable of some of the transformations they do not come up with all the features. Also, the ingestion might get delayed and make the flow unstable. Hence we need to use the tools which are built for performing transformations as the data is streamed. Here are some of the prominent tools.\n",
    "\n",
    "* Spark Streaming or Spark Structured Streaming (a module built as part of Spark)\n",
    "* Flink\n",
    "* Storm\n",
    "* Kafka Streams\n",
    "* and more\n",
    "\n",
    "***Databases***\n",
    "\n",
    "Once the data is processed, we have to store data in databases to persist and build a visualization layer on top of the processed data. We can use\n",
    "\n",
    "* RDBMS – such as Oracle, MySQL, Postgres etc\n",
    "* Data Warehouses – such as Teradata, Redshift etc\n",
    "* NoSQL databases – such as HBase, Cassandra, MongoDB, DynamoDB etc\n",
    "* Search based databases – such as Elastic Search\n",
    "\n",
    "***Visualization***\n",
    "\n",
    "Visualization is typically done as part of the application development using standard frameworks.\n",
    "\n",
    "* d3js\n",
    "* Kibana\n",
    "* Standard reporting tools such as Tableau\n",
    "* and more\n",
    "\n",
    "***Frameworks***\n",
    "\n",
    "As we discuss different moving parts in building streaming pipelines now let us get into frameworks. Most of these frameworks do not have visualization included.\n",
    "\n",
    "* Kafka\n",
    "    * Kafka Connect\n",
    "    * Kafka Topic\n",
    "    * Kafka Streams\n",
    "* ELK\n",
    "    * Elastic Search (Database)\n",
    "    * Logstash (streaming and processing logs)\n",
    "    * Kibana (Visualization)\n",
    "* HDF – Streaming services running behind NiFi\n",
    "* MapR Streams – Streaming services running on MapR cluster\n",
    "* AWS Services\n",
    "    * DynamoDB (Database)\n",
    "    * s3 (persistent storage of flat file format)\n",
    "    * Kinesis (streaming and processing logs)\n",
    "We have highlighted some of the popular frameworks. Almost all the top vendors such as Cloudera, Google, Microsoft Azure etc have necessary services to build streaming pipelines.\n",
    "\n",
    "### Spark Structured Streaming – Overview\n",
    "Apache Spark is a proven distributed computing framework with modules for different purposes\n",
    "\n",
    "* Core APIs – Transformations and Actions\n",
    "* Spark SQL and Data Frames\n",
    "* Spark Streaming (legacy) and Spark Structured Streaming\n",
    "* Spark MLLib\n",
    "* Spark GraphX\n",
    "* and more\n",
    "\n",
    "We can use Spark Structured Streaming to apply complex business rules either by using Data Frame operations or Spark SQL. Let us review Official Documentation to understand how it is structured.\n",
    "\n",
    "Typical batch job execution life cycle.\n",
    "\n",
    "* Create a Spark Context (Provision Resources)\n",
    "* Run the jobs or applications\n",
    "    * Read Data from Source (typically from files or databases)\n",
    "    * Apply Transformations\n",
    "        * Row Level Transformations\n",
    "        * Joining Data Sets\n",
    "        * Group and Perform Aggregations\n",
    "        * Sorting and Ranking\n",
    "        * Deduplication\n",
    "        * and more\n",
    "    * Write Data to Target/Sink (typically to files or databases)\n",
    "* Close Spark Context (Cleanup Resources)\n",
    "* We typically schedule jobs using enterprise scheduling tools.\n",
    "* This works fine if the frequency of the job is beyond an hour.\n",
    "\n",
    "Streaming Context for micro batches.\n",
    "\n",
    "* However, if we have to apply transformations in real time, then the overhead of creating and closing spark context in relative to data processing is considerably higher.\n",
    "* We can solve this problem by using the Streaming Context. It is created when we use spark.readStream. Instead of closing the context, it will keep on polling the source and read the data at regular intervals.\n",
    "* Context will be closed when we terminate it.\n",
    "\n",
    "***Important Concepts***\n",
    "\n",
    "Let us understand some of the important concepts related to Spark Structured Streaming. We have already seen spark.read to read the data and df.write to write the data while building batch data pipelines. For streaming pipelines, we have spark.readStream to read the data and df.writeStream to write the data in streaming fashion.\n",
    "\n",
    "* Sources\n",
    "    * File\n",
    "    * Kafka\n",
    "    * Socket (for testing)\n",
    "* Basic Operations or Transformations\n",
    "    * Row Level Transformations\n",
    "    * Joining Data Sets\n",
    "    * Group and Perform Aggregations\n",
    "    * Sorting and Ranking\n",
    "    * Deduplication\n",
    "    * and more\n",
    "* Window Operations on Event Time (will cover later)\n",
    "    * Handling late data and watermarking\n",
    "* Output Modes\n",
    "    * Append Mode\n",
    "    * Update Mode\n",
    "    * Complete Mode\n",
    "* Sinks/Targets\n",
    "    * Console\n",
    "    * File\n",
    "    * Memory\n",
    "    * Kafka\n",
    "    * foreach (can be used to write to Database)\n",
    "* Fault Tolerance and Offset Management\n",
    "\n",
    "### Setup Project\n",
    "Let us understand how to setup a project to build applications using Spark Structured Streaming.\n",
    "\n",
    "***Development Life Cycle***\n",
    "\n",
    "Let us first go through the details about the Development Life Cycle.\n",
    "\n",
    "* Make sure gen_logs is set up and data is being streamed\n",
    "* Create new project StreamingDemo using IntelliJ\n",
    "    * Choose scala 2.11\n",
    "    * Choose sbt 0.13.x\n",
    "    * Make sure JDK is chosen\n",
    "* Update build.sbt. See below\n",
    "* Define application properties\n",
    "* Create GetStreamingDepartmentTraffic object\n",
    "* Add logic to process data using Spark Structured Streaming\n",
    "* Build jar file\n",
    "* Ship to cluster and deploy\n",
    "\n",
    "***Dependencies (build.sbt)***\n",
    "\n",
    "Spark structured streaming require Spark SQL dependencies.\n",
    "\n",
    "* Add type safe config dependency so that we can externalize properties\n",
    "* Add spark-core and spark-sql dependencies\n",
    "* Replace build.sbt with below lines of code\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// StreamingDemo-build.sbt\n",
    "\n",
    "name := \"StreamingDemo\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.11.12\"\n",
    "\n",
    "libraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.2\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.3.0\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.3.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Externalize Properties***\n",
    "We need to make sure that the application can be run in different environments. It is very important to understand how to externalize properties and pass the information at run time.\n",
    "\n",
    "* Make sure build.sbt have dependency related to typesafe config\n",
    "* Create a new directory under src/main by name resources\n",
    "* Add a file called application.properties and add below entries\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// StreamingDemo-application.properties\n",
    "\n",
    "dev.execution.mode = local\n",
    "dev.data.host = localhost\n",
    "dev.data.port = 9999\n",
    "\n",
    "prod.execution.mode = yarn\n",
    "//make sure you use appropriate host for which you have access to\n",
    "prod.data.host = gw02.itversity.com\n",
    "prod.data.port = 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop logic using REPL\n",
    "Here are the steps involved in developing streaming applications.\n",
    "\n",
    "* Make sure to redirect the output of log file to localhost using port number 9999 (<mark>tail_logs.sh|nc -lk 9999</mark>)\n",
    "* Typical data processing life cycle\n",
    "    * Read Data – We can read data from files as well as tools like Kafka, Flume etc.\n",
    "    * Process Data – Once the data is read it can be processed using Data Frame Operations or Spark SQL.\n",
    "    * Write Data – We can write Data Frame to different sinks such as File, Kafka, Console, Memory, Database etc.\n",
    "* We will start with reading data from the file as well as Kafka and then look into other aspects. To  validate read is successful we will write into memory and run queries.\n",
    "\n",
    "***Reading Data***\n",
    "\n",
    "* **spark.readStream** is the higher level API to read data in streaming fashion. It is similar to **spark.read**\n",
    "* We can read the data either from files or from tools like Kafka, Flume etc.\n",
    "* To read data from files, either we can use APIs such as **spark.readStream.csv** and pass the path or we can use APIs such as **spark.readStream.format** where file format is passed as an argument.\n",
    "* Following are the file formats supported (same as **spark.read**)\n",
    "    * csv and text\n",
    "    * json\n",
    "    * orc\n",
    "    * parquet\n",
    "* When we try to read the data from files, we need to apply schema. Unlike in **spark.read**, by default schema inference is disabled. We can enable schema inference by setting **spark.sql.streaming.schemaInference** to true\n",
    "* We can also read data in streaming fashion from external web services or tools like kafka, flume etc using **spark.readStream.format**. We need to pass connectivity information using the option function.\n",
    "* Depending upon the format we need to set options (e.g.: host and port for socket)\n",
    "* Once we pass all the information, we can invoke the load function to create Data Frame."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// spark-structured-streaming-01-files.scala\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "  builder.\n",
    "  master(\"local\").\n",
    "  appName(\"Spark Structured Streaming Demo\").\n",
    "  getOrCreate\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "val orders = spark.\n",
    "  readStream.\n",
    "  schema(\"order_id INT, order_date STRING, order_customer_id INT, order_status STRING\").\n",
    "  csv(\"/mnt/c/data/retail_db/orders\")\n",
    "\n",
    "val query = orders.\n",
    "  writeStream.\n",
    "  queryName(\"orders\").\n",
    "  format(\"memory\").\n",
    "  start\n",
    "\n",
    "spark.sql(\"select * from orders\").show"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// spark-structured-streaming-02-socket.scala\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.\n",
    "  builder.\n",
    "  master(\"local\").\n",
    "  appName(\"Get Department Traffic\").\n",
    "  getOrCreate\n",
    "\n",
    "import spark.implicits._\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "val lines = spark.readStream.\n",
    "  format(\"socket\").\n",
    "  option(\"host\", \"localhost\").\n",
    "  option(\"port\", \"9999\").\n",
    "  load\n",
    "\n",
    "val departmentTraffic = lines.\n",
    "  where(split(split($\"value\", \" \")(6), \"/\")(1) === \"department\").\n",
    "  select(split(split($\"value\", \" \")(6), \"/\")(2).alias(\"department_name\")).\n",
    "  groupBy($\"department_name\").\n",
    "  agg(count($\"department_name\").alias(\"department_count\"))\n",
    "\n",
    "val query = departmentTraffic.\n",
    "  writeStream.\n",
    "  queryName(\"department_count\").\n",
    "  outputMode(\"complete\").\n",
    "  format(\"memory\").\n",
    "  start\n",
    "\n",
    "spark.sql(\"select * from department_count\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Processing Data***\n",
    "\n",
    "Let us see how we can process data using Data Frame Operations or Spark SQL. As both are covered extensively earlier, we will only see how either of the approaches can be used rather than diving deep into all aspects of Data Frame Operations or Spark SQL.\n",
    "\n",
    "* Once data is processed we can write the output into the specified target using **format** on top of **df.writeStream**.\n",
    "* We need to specify output mode (append, complete or update). By default, it is append. However, when aggregations are involved we can only use complete or update.\n",
    "* To print the data on the console in real time we need to use the console as part of the format.\n",
    " *Here is the complete code snippet, which read data in real time and then applies logic to get department count and print output every 20 seconds on the console.\n",
    " *For use cases like moving aggregations, we can use Window Operations.\n",
    " \n",
    "DATA FRAME OPERATIONS\n",
    "\n",
    "Let us see how we can process data using Data Frame Operations after creating Data Frame using readStream.\n",
    "* We need to use APIs such as select, withColumn to project the data.\n",
    "* We need to use APIs such as the filter or where to filter the data.\n",
    "* Aggregations can be performed using groupBy.\n",
    "* As part of this process, we need to use functions that are available under **org.apache.spark.sql.functions**. In our case, we have used functions like split, to_timestamp while filtering as well as projecting the data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// GetStreamingDepartmentTraffic-REPL-DataFrameOperations.scala\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "  builder.\n",
    "  master(\"local\").\n",
    "  appName(\"Get Department Traffic\").\n",
    "  getOrCreate\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "import spark.implicits._\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "val lines = spark.readStream.\n",
    "  format(\"socket\").\n",
    "  option(\"host\", \"localhost\").\n",
    "  option(\"port\", \"9999\").\n",
    "  load\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val departmentLines = lines.\n",
    "  where(split(split($\"value\", \" \")(6), \"/\")(1) === \"department\").\n",
    "  withColumn(\"department_name\", split(split($\"value\", \" \")(6), \"/\")(2)).\n",
    "  withColumn(\"visit_time\", to_timestamp(split($\"value\", \" \")(3), \"[dd/MMM/yyyy:HH:mm\")).\n",
    "  drop($\"value\")\n",
    "\n",
    "val departmentTraffic = departmentLines.\n",
    "  groupBy($\"visit_time\", $\"department_name\").\n",
    "  agg(count(\"department_name\").alias(\"department_count\"))\n",
    "\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "val query = departmentTraffic.\n",
    "  writeStream.\n",
    "  queryName(\"department_traffic\").\n",
    "  format(\"memory\").\n",
    "  outputMode(\"update\").\n",
    "  trigger(Trigger.ProcessingTime(\"20 seconds\")).\n",
    "  start\n",
    "\n",
    "spark.sql(\"SELECT * FROM department_traffic\").show(false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPARK SQL\n",
    "\n",
    "Let us see how we can process data using Spark SQL after creating Data Frame using readStream.\n",
    "* We first have to register Data Frame as a view.\n",
    "* Once Data Frame is registered as a view, we can develop SQL based query and pass it to spark.sql to process the data.\n",
    "* It will create a new Data Frame with processed data. We can write the Data Frame to target using relevant APIs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// GetStreamingDepartmentTraffic-REPL-SparkSQL.scala\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "  builder.\n",
    "  master(\"local\").\n",
    "  appName(\"Get Department Traffic\").\n",
    "  getOrCreate\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "import spark.implicits._\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "val lines = spark.readStream.\n",
    "  format(\"socket\").\n",
    "  option(\"host\", \"localhost\").\n",
    "  option(\"port\", \"9999\").\n",
    "  load\n",
    "\n",
    "lines.createTempView(\"lines\")\n",
    "\n",
    "val departmentTraffic = spark.sql(s\"\"\"\n",
    "  SELECT\n",
    "    to_timestamp(split(value, ' ')[3], '[dd/MMM/yyyy:HH:mm') visit_time,\n",
    "    split(split(value, ' ')[6], '/')[2] department_name, count(1) department_count\n",
    "  FROM lines\n",
    "  WHERE split(split(value, ' ')[6], '/')[1] = 'department'\n",
    "  GROUP BY \n",
    "    to_timestamp(split(value, ' ')[3], '[dd/MMM/yyyy:HH:mm'),\n",
    "    department_name\"\"\")\n",
    "\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "val query = departmentTraffic.\n",
    "  writeStream.\n",
    "  queryName(\"department_traffic\").\n",
    "  format(\"memory\").\n",
    "  outputMode(\"update\").\n",
    "  trigger(Trigger.ProcessingTime(\"20 seconds\")).\n",
    "  start\n",
    "\n",
    "spark.sql(\"SELECT * FROM department_traffic\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WINDOW OPERATIONS\n",
    "\n",
    "Let us see how we can process data using Data Frame Operations after creating Data Frame using readStream.\n",
    "* We need to use APIs such as select, withColumn to project the data.\n",
    "* We need to use APIs such as the filter or where to filter the data.\n",
    "* Aggregations can be performed using groupBy. We can pass window function as an argument along with the window interval as well as the slide interval as arguments.\n",
    "* As part of this process, we need to use functions that are available under **org.apache.spark.sql.functions**. In our case, we have used functions like split, to_timestamp while filtering as well as projecting the data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// GetStreamingDepartmentTraffic-REPL-WindowOperations.scala\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "  builder.\n",
    "  master(\"local\").\n",
    "  appName(\"Get Department Traffic\").\n",
    "  getOrCreate\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "import spark.implicits._\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "val lines = spark.readStream.\n",
    "  format(\"socket\").\n",
    "  option(\"host\", \"localhost\").\n",
    "  option(\"port\", \"9999\").\n",
    "  load\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val departmentLines = lines.\n",
    "  where(split(split($\"value\", \" \")(6), \"/\")(1) === \"department\").\n",
    "  withColumn(\"department_name\", split(split($\"value\", \" \")(6), \"/\")(2)).\n",
    "  withColumn(\"visit_time\", to_timestamp(ltrim(split($\"value\", \" \")(3), \"[\"), \"dd/MMM/yyyy:HH:mm:ss\")).\n",
    "  drop($\"value\")\n",
    "\n",
    "val departmentTraffic = departmentLines.\n",
    "  groupBy(window($\"visit_time\", \"60 seconds\", \"20 seconds\"), $\"department_name\").\n",
    "  agg(count(\"department_name\").alias(\"department_count\"))\n",
    "\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "val query = departmentTraffic.\n",
    "  writeStream.\n",
    "  queryName(\"department_traffic\").\n",
    "  format(\"memory\").\n",
    "  outputMode(\"update\").\n",
    "  trigger(Trigger.ProcessingTime(\"20 seconds\")).\n",
    "  start\n",
    "\n",
    "spark.sql(\"SELECT * FROM department_traffic\").show(false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Writing Data***\n",
    "\n",
    "As we have seen how to read data and process it using Data Frame Operations or Spark SQL, now let us see how to write the data back to a sink.\n",
    "* We can write the output to a different type of sinks or targets.\n",
    "    * file\n",
    "    * kafka\n",
    "    * memory\n",
    "    * console\n",
    "    * database using foreach\n",
    "* We can use writeStream.format to write data into file or memory or console or external plugins like Kafka. For Databases, we need to have a custom writer where we provide logic to open connection, process data and close the connection.\n",
    "* We need to specify outputMode while writing data to Sink. Valid modes are append, update and complete.\n",
    "update and complete are used on top of aggregated results while append is used on top of Data Frames which are processed using row level transformations.\n",
    "* We might not be able to use all 3 modes with every sink. For example, we will not be able to write data to files in a complete or update mode.\n",
    "* Go to this [link](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes) as part of official documentation to get the most relevant information about compatibility between output modes, sinks, and transformations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// spark-structured-streaming-write-to-file.scala\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "  builder.\n",
    "  master(\"local\").\n",
    "  appName(\"Get Department Traffic\").\n",
    "  getOrCreate\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "import spark.implicits._\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "val lines = spark.readStream.\n",
    "  format(\"socket\").\n",
    "  option(\"host\", \"localhost\").\n",
    "  option(\"port\", \"9999\").\n",
    "  load\n",
    "\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "lines.\n",
    "  writeStream.\n",
    "  format(\"csv\").\n",
    "  option(\"path\", \"/mnt/c/data/department_traffic\").\n",
    "  option(\"checkpointLocation\", \"/mnt/c/data/dtcp\").\n",
    "  trigger(Trigger.ProcessingTime(\"60 seconds\")).\n",
    "  start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Life Cycle – IDE\n",
    "As we explored Spark Structured Streaming APIs in detail, now let us understand how we can develop applications for Streaming Data Pipelines using IDE.\n",
    "* Create a scala program by choosing the Scala Class and then type Object\n",
    "* Make sure the program is named as GetStreamingDepartmentTraffic\n",
    "* First, we need to import necessary APIs\n",
    "* Develop necessary logic\n",
    "    * Get the properties from application.properties\n",
    "    * Create a spark session object by name spark\n",
    "    * Create stream using spark.readStream\n",
    "    * Process data using Data Frame Operations\n",
    "    * Write the output to console (in actual applications we write the output to the database)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// StreamingDemo-GetStreamingDepartmentTraffic.scala\n",
    "\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "import org.apache.spark.sql.functions.{split, to_timestamp, window, count}\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "object GetStreamingDepartmentTraffic {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = ConfigFactory.load.getConfig(args(0))\n",
    "    val spark = SparkSession.\n",
    "      builder.\n",
    "      master(conf.getString(\"execution.mode\")).\n",
    "      appName(\"Get Streaming Department Traffic\").\n",
    "      getOrCreate()\n",
    "\n",
    "    import spark.implicits._\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "    val lines = spark.\n",
    "      readStream.\n",
    "      format(\"socket\").\n",
    "      option(\"host\", conf.getString(\"data.host\")).\n",
    "      option(\"port\", conf.getString(\"data.port\")).\n",
    "      load\n",
    "\n",
    "    val departmentLines = lines.\n",
    "      filter(split(split($\"value\", \" \")(6), \"/\")(1) === \"department\").\n",
    "      withColumn(\"visit_time\", to_timestamp(split($\"value\", \" \")(3), \"[dd/MMM/yyyy:HH:mm:ss\")).\n",
    "      withColumn(\"department_name\", split(split($\"value\", \" \")(6), \"/\")(2)).\n",
    "      drop($\"value\")\n",
    "\n",
    "    val departmentTraffic = departmentLines.\n",
    "      groupBy(\n",
    "        window($\"visit_time\", \"60 seconds\", \"20 seconds\"),\n",
    "        $\"department_name\"\n",
    "      ).\n",
    "      agg(count(\"visit_time\").alias(\"department_count\"))\n",
    "\n",
    "    val query = departmentTraffic.\n",
    "      writeStream.\n",
    "      outputMode(\"update\").\n",
    "      format(\"console\").\n",
    "      trigger(Trigger.ProcessingTime(\"20 seconds\")).\n",
    "      start\n",
    "\n",
    "    query.awaitTermination()\n",
    "\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Build, Deploy and Run***\n",
    "\n",
    "Let us see how we can build and run the application locally and then on the cluster.\n",
    "* Right click on the project and copy path\n",
    "* Go to terminal and run cd command with the path copied\n",
    "* Run <mark>sbt package</mark>\n",
    "* It will generate a jar file for our application\n",
    "* Copy to the server where you want to deploy\n",
    "* Start streaming tail_logs to web service – <mark>tail_logs.sh|nc -lk gw02.itversity.com 9999</mark>\n",
    "* Run below command in another session on the server"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# StreamingDemo-GetStreamingDepartmentTraffic-using-jars.sh\n",
    "\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --class GetStreamingDepartmentTraffic \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  --jars \"/external_jars/config-1.3.2.jar\" \\\n",
    "  streamingdemo_2.11-1.0.jar prod"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# StreamingDemo-GetStreamingDepartmentTraffic-using-packages.sh\n",
    "\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --class GetStreamingDepartmentTraffic \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  --packages com.typesafe:config:1.3.2 \\\n",
    "  streamingdemo_2.11-1.0.jar prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Modes and Sinks\n",
    "Now let us talk review details with respect to Output Modes and Sinks.\n",
    "* Output Modes\n",
    "    * **Complete** – The entire updated Result Table will be written to the external storage. It is up to the storage connector to decide how to handle writing of the entire table.\n",
    "    * **Append** – Only the new rows appended in the Result Table since the last trigger will be written to the external storage. This is applicable only on the queries where existing rows in the Result Table are not expected to change.\n",
    "    * **Update** – Only the rows that were updated in the Result Table since the last trigger will be written to the external storage. If the query doesn’t contain aggregations, it will be equivalent to Append mode.\n",
    "* Sinks\n",
    "    * file\n",
    "    * kafka\n",
    "    * memory\n",
    "    * console\n",
    "    * foreach\n",
    "* Not all output modes are supported by all types of sinks and transformations. Go to this [link](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes)  as part of official documentation to get the most relevant information about compatibility between output modes, sinks, and transformations.\n",
    "* We need to implement ForeachWriter to use foreach to write data into target Databases.\n",
    "\n",
    "### Windowing and Handling Late Data\n",
    "Let us see Windowing and Handling Late Date using Watermarking.\n",
    "* In the previous example, we try to run every 20 seconds.\n",
    "* It will take complete data and perform aggregations every interval as there is no interval or window while grouping the data.\n",
    "* To actually run aggregations every interval, we need to either pass timestamp as part of the data or add timestamp while reading the data using <mark>option(\"includeTimestamp\", true)</mark>\n",
    "* We can also perform sliding windows such as a 10-minute window every 5 minutes.\n",
    "* When we use windowing, data might come late sometimes.\n",
    "* We can handle late data by using the concept of watermarking **(withWatermark).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
