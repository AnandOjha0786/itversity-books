{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBase Overview and Hive Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this topic, We will cover briefly on HBase and Hive.\n",
    "* Overview of HBase\n",
    "    * DDL and DML\n",
    "* Overview of Hive\n",
    "* Hive architecture\n",
    "* Hive properties and about log files\n",
    "\n",
    "### HBase\n",
    "HBase is a data model similar to Google’s big table that is designed to provide random access to the high volume of structured or unstructured data. HBase is an important component of the Hadoop ecosystem that leverages the fault tolerance feature of HDFS. HBase provides real-time read or writes access to data in HDFS. HBase can be referred to as a data store instead of a database as it misses out on some important features of traditional RDBMS like typed columns, triggers, advanced query languages and secondary indexes.HBase is not a direct replacement for a classic SQL database\n",
    "\n",
    "HBase is a NoSQL, column-oriented database built on top of Hadoop to overcome the drawbacks of HDFS as it allows fast random writes and reads in an optimized way. Also, with exponentially growing data, relational databases cannot handle the variety of data to render better performance. HBase region servers handle the data read in real time and written in real-time.n the HBase data model columns are grouped into column families, which must be defined up front during table creation. Column families are stored together on disk, which is why HBase is referred to as a column-oriented data store.\n",
    "\n",
    "### *[HBase Application – Development Life Cycle](https://kaizen.itversity.com/courses/)*\n",
    "* To get launch hbase we have to run the command hbase shell.\n",
    "* To get the help from the hbase shell just type help and we will get a bunch of commands and usage of them."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hbase shell"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The commands are categorised into\n",
    "    * DDL – Creating table, Altering table and dropping table.\n",
    "    * DML – Insert data, Update table etc.\n",
    "* To list all the tables we have to run below command"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A namespace is nothing but a logical grouping of tables.\n",
    "* Creating namespace in Hbase"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "create_namespace 'bootcampdemo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To list the tables under one namespace"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list_namespace_tables 'bootcampdemo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating a table under the namespace and insert a column under the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create 'bootcampdemo:demo','cf1'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "put 'bootcampdemo:demo',2,'cf1:c1', 'v1'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scan 'bootcampdemo:demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When we scan the table output will be sorted automatically\n",
    "* Put command will update the value based on the column name.\n",
    "* To list particular values based on key as the column"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scan 'bootcampdemo:demo',2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To delete one column\n",
    "* We can delete all the columns at a time by using delete all and column key"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "delete 'bootcampdemo:demo',2 'cf1:c1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* runcate to delete all the rows from the table\n",
    "* To get the available filters to use the command"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "show_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of Hive\n",
    "Hive is a flexible environment to create databases on HDFS. Hive uses HDFS as the data storage file system. Apache Hive is a data warehouse software project built on top of Apache Hadoop for providing data summarization, query, and analysis. Hive gives a SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop.\n",
    "* Metastore\n",
    "    * Stores the metadata for each of the tables such as their schema and location which is highly crucial.\n",
    "    * The data is stored in a traditional RDBMS format. The driver keeps the track of the metadata.\n",
    "* Driver\n",
    "    * The driver acts like a controller which receives the HiveQL statements.\n",
    "    * The driver also acts as a collection point of data or query result obtained after the Reduce operation.\n",
    "* Compiler\n",
    "    * The compiler performs a compilation of the HiveQL query, which converts the query to an execution plan.\n",
    "    * The compiler converts the query to an abstract syntax tree. \n",
    "* Optimizer\n",
    "    * The optimizer performs various transformations on the execution plan to get an optimized DAG.\n",
    "    * It can also split the tasks, such as applying a transformation to data before a reduce operation, to provide better performance and scalability.\n",
    "* Executor\n",
    "    * After compilation and optimization, the executor executes the tasks. It interacts with the job tracker of Hadoop to schedule tasks to be run.\n",
    "* CLI, UI, and Thrift Server\n",
    "    * A command-line interface (CLI) provides a user interface for an external user to interact with Hive by submitting queries, instructions and monitoring the process status.\n",
    "    * Thrift server allows external clients to interact with Hive over a network, similar to the JDBC or ODBC protocols.\n",
    "    \n",
    "### Hive CLI\n",
    "We can launch hive by just running simple command hive from any location in the gateway using **hive** command. Let us see how we can write queries.\n",
    "* The hive will launch its console where we can perform all the DDL and DML operations.\n",
    "* To list the databases"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "show databases;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To use the database and show tables us below commands"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "use database;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "show tables;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding Hive Properties\n",
    "We can alter run time behavior of Hive CLI by overriding the properties.\n",
    "* To get the properties, We can go to Ambari.\n",
    "* To check the properties from From the command line"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd /etc/hive/conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our system-level log file location"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd /tmp/`whoami`/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ls -ltr hive.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hive command line history file save at your home location with the name .hivehistory\n",
    "* To locate the hive warehouse directory"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set hive.metastore.warehouse.dir;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Set the hive execution engine to tez"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set hive.executionmode;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set hive.executionmode=tez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can update replication by command"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set dfs.replication;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dfs.replication=3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set dfs.replication=2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To control the behavior of the hive when launching we can write a file name .hiverc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vi .hiverc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set hive.executionmode=tez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also add properties from the command line by passing We can also add properties from the command line by passing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hive --hiveconf hive.executionmode=tez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To set the own log location, We can change at the time of launching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```hive --hiveconf```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```set hive.log.dir=/home/`whoami```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Hive in Spark Context\n",
    "Let us see how we can run Hive Queries using Spark Context.\n",
    "* We can access hive tables from the spark-sql console as well.\n",
    "* We can run the hive commands from spark-shell"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sqlContext.sql(\"Hive Query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hive has UI console which is available in Ambari, It is hive view."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - SQL",
   "language": "sql",
   "name": "apache_toree_sql"
  },
  "language_info": {
   "codemirror_mode": "text/x-sql",
   "file_extension": ".sql",
   "mimetype": "text/x-sql",
   "name": "sql",
   "pygments_lexer": "sql",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
