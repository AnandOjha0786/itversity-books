{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiveQL SortBy And Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this topic, We will understand sort by and different types of compressions.\n",
    "\n",
    "***Create NYSE table and loading the data***"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "create table nyse_data(\n",
    "stockticker string,\n",
    "tradedate string,\n",
    "openprice float,\n",
    "highprice float,\n",
    "lowprice float,\n",
    "closeprice float,\n",
    "volume bigint,\n",
    ") row format delimited fields terminated by ',';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data into table from local"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "load data local inpath '/data/nyse' into nyse_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Validating file blocks in HDFS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hdfs fsck /apps/hive/warehouse/dgadiraju_nyse.db/nyse_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To change the default number of reducers we can set it manually using below command"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set mapreduce.job.reduces=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For global sorting, We will use ORDER BY and it uses the number of final job’s reduce task as 1\n",
    "* Sorting nyse data using order by in descending order"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "select * from nyse_data\n",
    "ORDER BY tradedate, volume desc\n",
    "limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sorting nyse data using Sort by and creating table"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "create table nyse_data_sorted\n",
    "as \n",
    "select * from nyse_data \n",
    "distribute BY tradedate sort by tradedate,volume desc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is running only in one job.\n",
    "* The data has been divided by using hash mode algorithm and divided the data into three buckets.\n",
    "* The data have been sorted based on tradedate and volume.\n",
    "\n",
    "***Compression***\n",
    "* Loading compressed data into a table"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "truncate nyse_data;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "load data inpath local '/data/nyse_compressed' into nyse_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To list all the properties related to compressing we can use below command"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hive -e \"set;\" | grep compress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data will not be compressed unless we set the below property value to **true**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hive.exec.compress.output=true //default is false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To check all compression algorithms supported in your environment, look into **core-site.xml** and go to the property “io.compression.codecs”.\n",
    "* Source data might be compressed with right file format or uncompressed we can use load command.\n",
    "* If we want to compress the data and want to use insert command and return the compressed format we have to set the below property."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set hive.exec.compress.output=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To compress the insert data into Snappy we have to set the property and we have to give a fully qualified name, We can get it from the **core-site.xml**."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set hive.exec.compress.output=true"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "truncate table nyse_data_another;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set mapreduce.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "insert into nyse_data_another;"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "select * from nyse_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Compression types***\n",
    "* Uncompressed, Splittable and non-splittable\n",
    "    * Size of the file will differ\n",
    "    * But files will be stored in a similar manner\n",
    "    * If the file size is greater than 128 MB, the number of blocks will be generated using file size divided by the block size\n",
    "* Processing difference between Uncompressed, Splittable and non-splittable\n",
    "    * Will differ in the number of mappers\n",
    "    * Uncompressed and Splittable compression – the number of mappers will differ, but both leverage data locality using block size/split.\n",
    "    * Non Splittable compression will use only one mapper(no data locality).\n",
    "    * Splittable compressions – bzip2 and LZO\n",
    "    * Non-Splittable compressions – DEFLATE, gzip, LZ4, Snappy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - SQL",
   "language": "sql",
   "name": "apache_toree_sql"
  },
  "language_info": {
   "codemirror_mode": "text/x-sql",
   "file_extension": ".sql",
   "mimetype": "text/x-sql",
   "name": "sql",
   "pygments_lexer": "sql",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
