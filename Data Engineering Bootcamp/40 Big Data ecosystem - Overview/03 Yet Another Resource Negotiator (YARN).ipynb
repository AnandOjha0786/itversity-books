{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yet Another Resource Negotiator (YARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get into some of the important details with respect to YARN.\n",
    "\n",
    "* YARN Architecture\n",
    "* Running Map Reduce Jobs using YARN\n",
    "* Running Spark Jobs using YARN\n",
    "* YARN Application Life Cycle\n",
    "* Spark Job Execution Life Cycle\n",
    "* YARN Schedulers â€“ Overview\n",
    "\n",
    "Yet Another Resource Negotiator (YARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN Architecture\n",
    "\n",
    "YARN also follows Master-Slave Architecture. It is primarily used for Resource Management and Scheduling of Jobs.\n",
    "\n",
    "* YARN Components\n",
    "    * Resource Manager is the master in YARN\n",
    "    * Node Managers on worker nodes are slaves in YARN\n",
    "    * Node Managers collect usage information from respective nodes and send the details to the Resource Manager as part of the heartbeat\n",
    "    * Resource Manager keeps track of the usage of the cluster. You can review this information using Resource Manager UI.\n",
    "    * App timeline Server is to keep track of YARN applications submitted on the cluster\n",
    "* We can run Map Reduce Jobs as well as Spark Jobs using YARN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Map Reduce Jobs using YARN\n",
    "\n",
    "Let us run simple Map Reduce Job and see what happens. We will be using Hadoop examples that come as part of the setup process itself.\n",
    "\n",
    "* We can use Hadoop jar or yarn jar to submit map reduce job as YARN application. Let us run an application called randomtextwriter which will generate 10 GB of data per node by default.\n",
    "* This job will take some to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "hadoop jar \\\n",
    "  /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar \\\n",
    "  randomtextwriter \\\n",
    "  -Ddfs.replication=1 \\\n",
    "  /user/`whoami`/randomtextwriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Typically data will be processed using map tasks and reduce tasks.\n",
    "    * Map Tasks read the data and perform row-level transformations.\n",
    "    * Reduce Tasks read the output of Map Tasks and perform transformations such as joins, aggregations etc.\n",
    "    * Shuffling Process between Map Tasks and Reduce Tasks take care of grouping and partitioning of data based on keys.\n",
    "    * We do not have to get into too many details at this time as an administrator.\n",
    "    * This particular application **randomtextwriter** is map only job where it tries to create 10 GB data per data node. In our case, we will see 30 GB of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise***: Run relevant Hadoop fs commands to get the size of data that is created by randomtextwriter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Map tasks and Reduce tasks will run as part of YARN containers on Nodemanagers.\n",
    "* The life cycle of the job is managed by per job application master.\n",
    "* Typically Map Reduce jobs read data from HDFS, process it and save it back to HDFS.\n",
    "* This examples job does not take any data from HDFS, it just randomly generates text and writes it back to HDFS.\n",
    "\n",
    "We can keep track of running jobs as well as troubleshoot completed jobs using Resource Manager UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Spark Jobs using YARN\n",
    "\n",
    "Spark provides APIs as well as Framework for distributed processing.\n",
    "\n",
    "* Developers take care of developing Spark based applications using Scala or Python or Java.\n",
    "* When the code is released, it is the responsibility of Developers to provide a run guide for their applications.\n",
    "* As part of Spark setup, we get examples and they can be submitted using the spark-submit command. Let us review some of the arguments we can pass using spark-submit to control the runtime behavior of Spark Application.\n",
    "* We can also launch Scala REPL with Spark dependencies using spark-shell and Python CLI with Spark dependencies using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste in the Terminal\n",
    "spark-submit --master yarn \\\n",
    "  --conf spark.ui.port=0 \\\n",
    "  --class org.apache.spark.examples.JavaWordCount \\\n",
    "  /usr/lib/spark/lib/spark-examples.jar \\\n",
    "  /user/itversity/randomtextwriter/part-m-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste in the Terminal\n",
    "spark2-shell \\\n",
    "  --master yarn \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  --conf spark.dynamicAllocation.enabled=false \\\n",
    "  --num-executors 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once console is launched in terminal, copy and paste the below code\n",
    "\n",
    "sc.textFile(\"/user/itversity/randomtextwriter/part-m-00000\").\n",
    "  flatMap(_.split(\" \")).\n",
    "  map((_, 1)).\n",
    "  reduceByKey(_ + _).\n",
    "  saveAsTextFile(\"/user/itversity/spark-shell/wordcount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste in the Terminal\n",
    "pyspark2 \\\n",
    "  --master yarn \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  --conf spark.dynamicAllocation.enabled=false \\\n",
    "  --num-executors 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once console is launched in terminal, copy and paste the below code\n",
    "sc.textFile(\"/user/itversity/randomtextwriter/part-m-00000\"). \\\n",
    "  flatMap(lambda line: line.split(\" \")). \\\n",
    "  map(lambda word: (word, 1)). \\\n",
    "  reduceByKey(lambda x, y: x + y). \\\n",
    "  saveAsTextFile(\"/user/itversity/pyspark/wordcount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After running the jobs let us also review UI to monitor either running or completed jobs.\n",
    "* Here Spark is integrated with YARN and hence Spark Job or Application is nothing but YARN Application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN Application Life Cycle\n",
    "\n",
    "Now let us talk about YARN Application Life Cycle. YARN is the resource management framework.\n",
    "\n",
    "* We can use distributed data processing frameworks such as Map Reduce, Spark etc., by plugging into YARN.\n",
    "* A YARN application can be Map Reduce Job or Spark Application.\n",
    "* From YARN perspective data is being processed by containers.\n",
    "* Let us understand the life cycle of YARN Application.\n",
    "    * We use the client to submit YARN Application (for e. g.: Map Reduce Job)\n",
    "    * The request will go to the Resource Manager. Resource Manager has up to date information about the usage of all the servers on registered Node Managers running on servers.\n",
    "    * Resource Manager will decide a node on which container should run to manage the job or application using different criteria such as usage of the servers.\n",
    "    * This container is called as Application Master. It will be up and running until the application is either completed or killed.\n",
    "    * Now Application Master will talk to Node Managers directly and decide on which nodes containers should run to process the data. It uses Data Locality and Server Usage as criteria before creating containers.\n",
    "    * These containers will process the data and might get garbage collected depending upon the underlying data processing framework.\n",
    "\n",
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/10/09YARNApplicationLifeCycle-1024x1024.png)\n",
    "\n",
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/10/11YARNLifeCycle-1024x956.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Job Execution Life Cycle\n",
    "\n",
    "Let us understand the Execution Life Cycle of Spark. You can review this using [Spark Official Documentation](https://spark.apache.org/docs/latest/cluster-overview.html).\n",
    "\n",
    "* We submit the job for the client. The JVM typically acts as the Driver Program.\n",
    "* It will talk to the Resource Manager and create the Application Master.\n",
    "* Application Master will talk to Worker Nodes on which Node Managers are running and provision resources based on Allocation Settings. Allocation can be either static or dynamic.\n",
    "* These resources are nothing but Executors. From YARN perspective they are Containers.\n",
    "* The Executor is nothing but JVM which can run multiple concurrent threads until the Job is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste in the Terminal\n",
    "spark-submit --master yarn \\\n",
    "  --conf spark.ui.port=0 \\\n",
    "  --class org.apache.spark.examples.JavaWordCount \\\n",
    "  /usr/lib/spark/lib/spark-examples.jar \\\n",
    "  /user/itversity/randomtextwriter/part-m-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste in the Terminal\n",
    "spark2-shell \\\n",
    "  --master yarn \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  --conf spark.dynamicAllocation.enabled=false \\\n",
    "  --num-executors 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once console is launched in terminal, copy and paste the below code\n",
    "\n",
    "sc.textFile(\"/user/itversity/randomtextwriter/part-m-00000\").\n",
    "  flatMap(_.split(\" \")).\n",
    "  map((_, 1)).\n",
    "  reduceByKey(_ + _).\n",
    "  saveAsTextFile(\"/user/itversity/spark-shell/wordcount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste in the Terminal\n",
    "pyspark2 \\\n",
    "  --master yarn \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  --conf spark.dynamicAllocation.enabled=false \\\n",
    "  --num-executors 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once console is launched in terminal, copy and paste the below code\n",
    "sc.textFile(\"/user/itversity/randomtextwriter/part-m-00000\"). \\\n",
    "  flatMap(lambda line: line.split(\" \")). \\\n",
    "  map(lambda word: (word, 1)). \\\n",
    "  reduceByKey(lambda x, y: x + y). \\\n",
    "  saveAsTextFile(\"/user/itversity/pyspark/wordcount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YARN Schedulers â€“ Overview\n",
    "\n",
    "Let us see details about Schedulers in YARN.\n",
    "\n",
    "* YARN primarily take care of Resource Management and Job Scheduling.\n",
    "* There are three types of Schedulers supported by YARN. It is all about how the jobs and tasks associated with respect to the jobs are queued up as they execute.\n",
    "    * FIFO Scheduler\n",
    "    * Fair Scheduler\n",
    "    * Capacity Scheduler\n",
    "* FIFO Scheduler is default in Plain Vanilla Hadoop.\n",
    "* In Cloudera Distribution, Fair Scheduler is default.\n",
    "* With FIFO Scheduler, jobs are given priority in the order of submission.\n",
    "* With Fair Scheduler as resources are freed up, all the job tasks in queue will get equal priority.\n",
    "* It is very easy to switch the schedulers using management tools by administrators.\n",
    "* Fair Scheduler and Capacity Scheduler have pools and queues which need to be configured so that resources are distributed between different categories of applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
