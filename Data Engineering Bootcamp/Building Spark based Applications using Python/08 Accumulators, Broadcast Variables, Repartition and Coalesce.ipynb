{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accumulators, Broadcast Variables, Repartition and Coalesce\n",
    "\n",
    "Now let us understand about Accumulators and Broadcast Variables. They are also known as Shared Variables. Accumulators are primarily used as counters for sanity checks while broadcast variables are used for lookups. As part of this topic, we will also look into repartition and coalesce.\n",
    "\n",
    "* Accessing HDFS APIs using sc in Python\n",
    "* Validating input paths and output paths leveraging HDFS APIs\n",
    "* Perform join between orders and order_items from HDFS\n",
    "* Convert data from local file into RDD and then join to get Product Name\n",
    "* Use accumulators to get number of orders and number of order items processed\n",
    "* Running on cluster and check the accumulators as part of UI\n",
    "* Develop alternative solution using Broadcast variables\n",
    "* Read products from local file and create dict\n",
    "* Change join on products to lookup using broadcast variable of type dict\n",
    "* Run on cluster and check the behavior\n",
    "* Understand the relevance and syntax of repartition as well as coalesce\n",
    "\n",
    "In this session we will develop a program using HDFS APIs and then add accumulators to it.\n",
    "\n",
    "You will not be able to see accumulator details as part of Spark UI for pyspark applications. However, you can read the variables after performing the action within the program. We have struggled a bit in the video, but the code is updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Before going into shared variables such as Accumulators and Broadcast Variables, let us define a new problem statement and come up with the solution. With this, we will understand how we can use HDFS APIs as part of applications built using Pyspark.\n",
    "\n",
    "\n",
    "* We have to use orders, order_items and products data set to compute revenue per product for a given month\n",
    "* orders have order_id and order_date\n",
    "* order_items have order_item_subtotal, order_item_order_id and order_item_product_id\n",
    "* products have product_id and product_name\n",
    "* orders and order_items are in HDFS and products is in local file system\n",
    "* High level design\n",
    " * Accept year and month as program argument (along with input path and output path)\n",
    " * Filter for orders which fall in the month passed as argument\n",
    " * Join filtered orders and order_items to get order_item details for a given month\n",
    " * Get revenue for each product_id\n",
    " * We need to read products from local file system\n",
    " * Convert into RDD and extract product_id and product_name\n",
    " * Join it with aggregated order_items (product_id, revenue)\n",
    " * Get product_name and revenue for each product\n",
    " * We will also check whether input path is valid and if output path exists we will delete it using HDFS APIs.\n",
    "* application.properties"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pyspark-application.properties\n",
    "\n",
    "[dev]\n",
    "executionMode = local\n",
    "\n",
    "[prod]\n",
    "executionMode = yarn-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create new package by name retail\n",
    "* Create Python program with name RevenuePerProductForMonth\n",
    "* src/main/python/retail/RevenuePerProductForMonth.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pyspark-RevenuePerProductForMonth.py \n",
    "\n",
    "import sys\n",
    "import configparser as cp\n",
    "try:\n",
    "    from pyspark import SparkConf, SparkContext\n",
    "    from pyspark.sql import SQLContext, Row, functions as func\n",
    "\n",
    "    props = cp.RawConfigParser()\n",
    "    props.read(\"src/main/resources/application.properties\")\n",
    "\n",
    "    conf = SparkConf(). \\\n",
    "    setAppName(\"Total Revenue Per Day\"). \\\n",
    "    setMaster(props.get(sys.argv[5], \"executionMode\"))\n",
    "\n",
    "    sc = SparkContext(conf=conf)\n",
    "    inputPath = sys.argv[1]\n",
    "    outputPath = sys.argv[2]\n",
    "    month = sys.argv[3]\n",
    "\n",
    "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    Configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "\n",
    "    fs = FileSystem.get(Configuration())\n",
    "\n",
    "    if(fs.exists(Path(inputPath)) == False):\n",
    "        print(\"Input path does not exists\")\n",
    "    else:\n",
    "        if(fs.exists(Path(outputPath))):\n",
    "            fs.delete(Path(outputPath), True)\n",
    "\n",
    "        # Filter for orders which fall in the month passed as argument\n",
    "        orders = inputPath + \"/orders\"\n",
    "        ordersFiltered = sc.textFile(orders). \\\n",
    "        filter(lambda order: month in order.split(\",\")[1]). \\\n",
    "        map(lambda order: (int(order.split(\",\")[0]), 1))\n",
    "\n",
    "        # Join filtered orders and order_items to get order_item details for a given month\n",
    "        # Get revenue for each product_id\n",
    "\n",
    "        orderItems = inputPath + \"/order_items\"\n",
    "        revenueByProductId = sc.textFile(orderItems). \\\n",
    "            map(lambda orderItem:\n",
    "                (int(orderItem.split(\",\")[1]), \n",
    "                 (int(orderItem.split(\",\")[2]), float(orderItem.split(\",\")[4])\n",
    "                ))\n",
    "            ). \\\n",
    "            join(ordersFiltered). \\\n",
    "            map(lambda rec: rec[1][0]). \\\n",
    "            reduceByKey(lambda total, ele: total + ele)\n",
    "\n",
    "        # We need to read products from local file system\n",
    "        localPath = sys.argv[4]\n",
    "        productsFile = open(localPath + \"/products/part-00000\")\n",
    "        products = productsFile.read().splitlines()\n",
    "\n",
    "        # Convert into RDD and extract product_id and product_name\n",
    "        # Join it with aggregated order_items (product_id, revenue)\n",
    "        # Get product_name and revenue for each product\n",
    "        sc.parallelize(products). \\\n",
    "            map(lambda product: \n",
    "                (int(product.split(\",\")[0]), product.split(\",\")[2])). \\\n",
    "        join(revenueByProductId). \\\n",
    "        map(lambda rec: rec[1][0] + \"\\t\" + str(rec[1][1])). \\\n",
    "        saveAsTextFile(outputPath)\n",
    "\n",
    "        print (\"Successfully imported Spark Modules\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print (\"Can not import Spark Modules\", e)\n",
    "sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the spark job using spark-submit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# spark-submit-python-revenueperproductformonth.sh\n",
    "\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --conf spark.ui.port=54123 \\\n",
    "  src/main/python/retail/RevenuePerProductForMonth.py \\\n",
    "  /public/retail_db /user/dgadiraju/revenueperproductformonth \\\n",
    "  2013-12 /data/retail_db prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulators\n",
    "\n",
    "Accumulators is a shared variable which can be used to implement counters with in the Spark application.\n",
    "\n",
    "* It is important to perform some counts as part of the application for\n",
    " * unit testing\n",
    " * data quality\n",
    "* These counters cannot be global variables as part of the program\n",
    "* Instead we need to use accumulator which will be managed by spark\n",
    "* Accumulators will be passed to executors and scope is managed across all the executors or executor tasks\n",
    "* Accumulators can be used in any Spark APIs\n",
    " * sc.accumulator() is the API to create accumulator\n",
    " * In any Spark API, we can increment the accumulator\n",
    " * As Python lambda functions directly cannot increment accumulator we will create a function which will be invoked as part of the Spark API such as map, filter etc.\n",
    "* Take **Revenue per product for given month** program and add below accumulators\n",
    " * To get number of orders for the month – ordersCount\n",
    " * To get number of orderItems for the month – orderItemsCount\n",
    " * Increment ordersCount as part of map function after filtering on the month\n",
    " * Increment orderItemsCount as part of map function after join\n",
    " * Print the results after action is performed. Until then no processing will be done over accumulators.\n",
    " * Typically we preserve this information in database or log files so that we can keep track of this important information.\n",
    "* Here are some of the known issues with accumulators\n",
    " * Unless tasks are executed you will not see details about counters\n",
    " * Spark guarantees accumulators to be updated only in first execution\n",
    " * If any task is re-executed the results can be inconsistent\n",
    " * The issue is prevalent in both transformations and actions\n",
    " * You will not be able to see accumulator details as part of Spark UI for pyspark applications. However, you can read the variables after performing the action with in the program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pyspark-RevenuePerProductForMonthAccumulator.py\n",
    "\n",
    "import sys\n",
    "import ConfigParser as cp\n",
    "try:\n",
    "    from pyspark import SparkConf, SparkContext\n",
    "    from pyspark.sql import SQLContext, Row, functions as func\n",
    "\n",
    "    props = cp.RawConfigParser()\n",
    "    props.read(\"src/main/resources/application.properties\")\n",
    "\n",
    "    conf = SparkConf(). \\\n",
    "    setAppName(\"Total Revenue Per Day\"). \\\n",
    "    setMaster(props.get(sys.argv[5], \"executionMode\"))\n",
    "\n",
    "    sc = SparkContext(conf=conf)\n",
    "    inputPath = sys.argv[1]\n",
    "    outputPath = sys.argv[2]\n",
    "    month = sys.argv[3]\n",
    "\n",
    "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    Configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "\n",
    "    fs = FileSystem.get(Configuration())\n",
    "\n",
    "    if(fs.exists(Path(inputPath)) == False):\n",
    "        print(\"Input path does not exists\")\n",
    "    else:\n",
    "        if(fs.exists(Path(outputPath))):\n",
    "            fs.delete(Path(outputPath), True)\n",
    "\n",
    "        # Filter for orders which fall in the month passed as argument\n",
    "        ordersCount = sc.accumulator(0)\n",
    "        orders = inputPath + \"/orders\"\n",
    "\n",
    "        def getOrdersTuples(rec):\n",
    "            ordersCount.add(1)\n",
    "            return (int(rec.split(\",\")[0]), 1)\n",
    "        ordersFiltered = sc.textFile(orders). \\\n",
    "        filter(lambda order: month in order.split(\",\")[1]). \\\n",
    "        map(getOrdersTuples)\n",
    "\n",
    "        # Join filtered orders and order_items to get order_item details for a given month\n",
    "        # Get revenue for each product_id\n",
    "        orderItemsCount = sc.accumulator(0)\n",
    "        orderItems = inputPath + \"/order_items\"\n",
    "\n",
    "        def getProductIdAndRevenue(rec):\n",
    "            orderItemsCount.add(1)\n",
    "            return rec[1][0]\n",
    "\n",
    "        revenueByProductId = sc.textFile(orderItems). \\\n",
    "            map(lambda orderItem:\n",
    "                (int(orderItem.split(\",\")[1]), \n",
    "                 (int(orderItem.split(\",\")[2]), float(orderItem.split(\",\")[4])\n",
    "                ))\n",
    "            ). \\\n",
    "            join(ordersFiltered). \\\n",
    "            map(getProductIdAndRevenue). \\\n",
    "            reduceByKey(lambda total, ele: total + ele)\n",
    "\n",
    "        # We need to read products from local file system\n",
    "        localPath = sys.argv[4]\n",
    "        productsFile = open(localPath + \"/products/part-00000\")\n",
    "        products = productsFile.read().splitlines()\n",
    "\n",
    "        # Convert into RDD and extract product_id and product_name\n",
    "        # Join it with aggregated order_items (product_id, revenue)\n",
    "        # Get product_name and revenue for each product\n",
    "        sc.parallelize(products). \\\n",
    "            map(lambda product: \n",
    "                (int(product.split(\",\")[0]), product.split(\",\")[2])). \\\n",
    "        join(revenueByProductId). \\\n",
    "        map(lambda rec: rec[1][0] + \"\\t\" + str(rec[1][1])). \\\n",
    "        saveAsTextFile(outputPath)\n",
    "        \n",
    "        # We can see the details of accumulators after performing action\n",
    "        # We typically stored this data into database\n",
    "        print(ordersCount)\n",
    "        print(orderItemsCount)\n",
    "except ImportError as e:\n",
    "    print (\"Can not import Spark Modules\", e)\n",
    "sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can run the job and see the details about accumulators as part of the logs in Spark UI/History UI\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# spark-submit-python-revenueperproductformonthaccumulator.sh\n",
    "\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --conf spark.ui.port=54123 \\\n",
    "  src/main/python/retail/RevenuePerProductForMonthAccumulator.py \\\n",
    "  /public/retail_db /user/dgadiraju/revenueperproductformonth \\\n",
    "2013-12 /data/retail_db prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this session, we will talk about Broadcast Variables along with repartition and coalesce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Variables\n",
    "\n",
    "Broadcast Variable is another type of shared variable which can be broadcasted into all the executors and can access at runtime by tasks while processing data. It is typically used to replace joins with lookups when a very large data set is joined with small data set which can fit into the memory of executor JVM.\n",
    "\n",
    "* At times we need to pass (broadcast) some information to all the executors\n",
    "* It can be done by using broadcast variables\n",
    "* A broadcast variable can be of preliminary type or it could be a hash map\n",
    "* Here are few examples\n",
    " * Single value – Common discount percent for all the products\n",
    " * Hash map – look up or map side join\n",
    "* When very large data set (fact) is tried to join with smaller data set (dimension), broadcasting dimension can have considerable performance improvement.\n",
    "* Broadcast variables are immutable\n",
    "* We can read data from HDFS or local file system or even as configuration parameters\n",
    "* Broadcast using broadcast method of Spark Context\n",
    "* Let us take the example of Revenue per product for a given month\n",
    "* Earlier we have read products from local file system, converted into RDD and then join with other RDD to get product name and revenue generated. Here is the DAG when the data is joined with out using broadcast variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/07/DAGRegularJoin.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is how DAG look like after broadcasting products from local file system. If we run this against considerable amount of data, one can feel the difference in the performance because of broadcast variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/07/DAGBroadCast-300x180.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is the code for broadcast variables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pyspark-RevenuePerProductForMonthBroadcast.py\n",
    "\n",
    "import sys\n",
    "import ConfigParser as cp\n",
    "try:\n",
    "    from pyspark import SparkConf, SparkContext\n",
    "    from pyspark.sql import SQLContext, Row, functions as func\n",
    "\n",
    "    props = cp.RawConfigParser()\n",
    "    props.read(\"src/main/resources/application.properties\")\n",
    "\n",
    "    conf = SparkConf(). \\\n",
    "    setAppName(\"Total Revenue Per Day\"). \\\n",
    "    setMaster(props.get(sys.argv[5], \"executionMode\"))\n",
    "\n",
    "    sc = SparkContext(conf=conf)\n",
    "    inputPath = sys.argv[1]\n",
    "    outputPath = sys.argv[2]\n",
    "    month = sys.argv[3]\n",
    "\n",
    "    Path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "    FileSystem = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    Configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "\n",
    "    fs = FileSystem.get(Configuration())\n",
    "\n",
    "    if(fs.exists(Path(inputPath)) == False):\n",
    "        print(\"Input path does not exists\")\n",
    "    else:\n",
    "        if(fs.exists(Path(outputPath))):\n",
    "            fs.delete(Path(outputPath), True)\n",
    "\n",
    "        # Filter for orders which fall in the month passed as argument\n",
    "        ordersCount = sc.accumulator(0)\n",
    "        orders = inputPath + \"/orders\"\n",
    "\n",
    "        def getOrdersTuples(rec):\n",
    "            ordersCount.add(1)\n",
    "            return (int(rec.split(\",\")[0]), 1)\n",
    "        \n",
    "        ordersFiltered = sc.textFile(orders). \\\n",
    "        filter(lambda order: month in order.split(\",\")[1]). \\\n",
    "        map(getOrdersTuples)\n",
    "\n",
    "        # Join filtered orders and order_items to get order_item details for a given month\n",
    "        # Get revenue for each product_id\n",
    "        orderItemsCount = sc.accumulator(0)\n",
    "        orderItems = inputPath + \"/order_items\"\n",
    "\n",
    "        def getProductIdAndRevenue(rec):\n",
    "            orderItemsCount.add(1)\n",
    "            return rec[1][0]\n",
    "        \n",
    "        revenueByProductId = sc.textFile(orderItems). \\\n",
    "            map(lambda orderItem:\n",
    "                (int(orderItem.split(\",\")[1]), \n",
    "                 (int(orderItem.split(\",\")[2]), float(orderItem.split(\",\")[4])\n",
    "                ))\n",
    "            ). \\\n",
    "            join(ordersFiltered). \\\n",
    "            map(getProductIdAndRevenue). \\\n",
    "            reduceByKey(lambda total, ele: total + ele)\n",
    "\n",
    "        # We need to read products from local file system\n",
    "        localPath = sys.argv[4]\n",
    "        productsFile = open(localPath + \"/products/part-00000\")\n",
    "        products = productsFile.read().splitlines()\n",
    "\n",
    "        # Extract product_id and product_name and create dict of it\n",
    "        # Broadcast the dict\n",
    "        productsDict = dict(\n",
    "            map(lambda product:\n",
    "                (int(product.split(\",\")[0]), product.split(\",\")[2]), products)\n",
    "        )\n",
    "        bv = sc.broadcast(productsDict)\n",
    "\n",
    "        # Get product name for each product id in revenueByProductId\n",
    "        # by looking up in the broadcast variable\n",
    "\n",
    "        revenueByProductId. \\\n",
    "        map(lambda product: bv.value[product[0]] + \"\\t\" + str(product[1])). \\\n",
    "        saveAsTextFile(outputPath)\n",
    "\n",
    "except ImportError as e:\n",
    "    print (\"Can not import Spark Modules\", e)\n",
    "sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is the code for submitting the job"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# spark-submit-python-revenueperproductformonthbroadcast.sh\n",
    "\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --conf spark.ui.port=54123 \\\n",
    "  src/main/python/retail/RevenuePerProductForMonthBroadcast.py \\\n",
    "  /public/retail_db /user/dgadiraju/revenueperproductformonth \\\n",
    "2013-12 /data/retail_db prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repartition and coalesce\n",
    "\n",
    "Now let us understand how we can control number of tasks to process data after first stage.\n",
    "\n",
    "* Each of the APIs which result in shuffling have additional argument numKeys or numPartitions\n",
    "* We can use repartition to control the number of tasks or partitions in subsequent stages\n",
    "* repartition result in shuffling process\n",
    "* We can increase or decrease number of partitions in RDD using repartition\n",
    "* coalesce can only be used to reduce number of partitions\n",
    "* coalesce does not result in shuffling process\n",
    "* Here is the example which covers coalesce and repartition (repartition will be slow in this case as it generate additional stage for shuffling data into new partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"yarn\"))\n",
    "lines = sc.textFile(\"/public/randomtextwriter/part-m-00000\", 18)\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "wordTuples = words.map(lambda word: (word, 1))\n",
    "wc = wordTuples.reduceByKey(lambda x, y: x + y, 36)\n",
    "wc.coalesce(4).saveAsTextFile(\"/user/training/bootcamp/pyspark/wordcount02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"/public/randomtextwriter/part-m-00000\", 18)\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "wordTuples = words.map(lambda word: (word, 1))\n",
    "wc = wordTuples.reduceByKey(lambda x, y: x + y, 36)\n",
    "wc.repartition(4).saveAsTextFile(\"/user/training/bootcamp/pyspark/wordcount03\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
