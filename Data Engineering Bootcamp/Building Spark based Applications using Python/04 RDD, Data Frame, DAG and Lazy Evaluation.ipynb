{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD, Data Frame, DAG and Lazy Evaluation\n",
    "\n",
    "Now let us see details about data structures in Spark such as Resilient Distributed Datasets, Data Frames, Directed Acyclic Graph, Lazy Evaluation etc.\n",
    "\n",
    "* Difference between Python list and RDD\n",
    "* Resilient Distributed Datasets\n",
    "* Data Frames\n",
    "* Overview of Transformations and Actions\n",
    "* Directed Acyclic Graph and Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between Python list and RDD\n",
    "Let us create simple RDD and compare with Python list.\n",
    "\n",
    "* Python list is nothing but a heap of elements.\n",
    "* We can manipulate Python list using APIs such as map, filter, reduce, set, sort, sorted etc.\n",
    "* Python list is typically processed in a linear fashion\n",
    "* RDD stands for Resilient Distributed Dataset. It is nothing but a distributed list.\n",
    "* RDD is a distributed collection or list provided as part of Spark.\n",
    "* APIs on RDD facilitate us to process it in a distributed fashion.\n",
    "* We can manipulate Spark RDD using RDD functions such as map, flatMap, filter, reduceByKey etc.\n",
    "* Spark gives us the desired scalability as APIs will process RDD in distributed fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets\n",
    "Resilient Distributed Datasets (in short RDD) is the fundamental data structure in Spark.\n",
    "\n",
    "### Creation of RDD\n",
    "\n",
    "Let us see how we can create RDD.\n",
    "\n",
    "* On top of SparkContext (sc) we have several APIs to create RDD using data from files.\n",
    " * textFile\n",
    " * sequenceFile\n",
    " * Hadoop related APIs\n",
    "* These APIs will not trigger execution immediately. They will update the DAG and we need to perform actions to trigger execution.\n",
    "* We can use APIs such as **take(10)** to preview the data from RDD. We cannot perform traditional list operations such **[:10]** on RDD.\n",
    "* Characteristics of RDD\n",
    " * In-memory\n",
    " * Distributed\n",
    " * Resilient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Life Cycle\n",
    "\n",
    "* Data from files will be divided into RDD partitions and each partition is processed by a separate task\n",
    "* By default, it will use HDFS block size (128 MB) to determine the partition si\n",
    "* We can increase (cannot decrease) number of partitions by using an additional parameter in sc.textFile\n",
    "* By default when data is loaded into memory each record will be serialized into Java object\n",
    "\n",
    "### RDD Persistence\n",
    "\n",
    "Typically data will not be loaded into memory immediately when we create RDD as part of the program. It will be processed in real time by loading data into memory as it is processed. If we have to retain RDD in memory for an extended period of time, then we have to use RDD Persistence.\n",
    "\n",
    "* Let us see what happens when RDD is loaded into memory\n",
    " * Serialize into Java Objects\n",
    " * Get into memory\n",
    " * As data is processed RDD  partitions will be flushed out of memory as tasks are completed.\n",
    "* We can persist the RDD partitions at different storage levels\n",
    " * MEMORY_ONLY (default)\n",
    " * MEMORY_AND_DISK\n",
    " * DISK_ONLY\n",
    " * and more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frames\n",
    "Many times data will have structure. Using RDD and then core APIs is some what tedious and cryptic. We can use Data Frames to address these issues. Here are the some of the advantages using Data Frames\n",
    "\n",
    "* Flexible APIs (Data Frame native operations as well as SQL)\n",
    "* Code will be readable\n",
    "* Better organized and manageable\n",
    "* Uses latest optimizers\n",
    "* Process data in binary format\n",
    "* Can generate execution plans based on statistics collected (for permanent tables such as Hive tables)\n",
    "\n",
    "We will talk about processing data using Data Frames in the next chapter. For now, we will be focusing on Core APIs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Transformations and Actions\n",
    "\n",
    "Spark Core APIs are categorized into Transformations and Actions. Let us explore them at a higher level.\n",
    "\n",
    "* Transformations\n",
    " * Row level transformations – map, flatMap, filter\n",
    " * Joins – join, leftOuterJoin, rightOuterJoin\n",
    " * Aggregations – reduceByKey, aggregateByKey\n",
    " * Sorting data – sortByKey\n",
    " * Group operations such as ranking – groupByKey\n",
    " * Set operations – union, intersection\n",
    " * and more\n",
    "* Actions\n",
    " * Previewing data – first, take, takeSample\n",
    " * Converting RDD into the typical collection – collect\n",
    " * Total aggregations – count, reduce\n",
    " * Total ranking – top\n",
    " * Saving files – saveAsTextFile, saveAsNewAPIHadoopFile etc\n",
    " * and more\n",
    " \n",
    "Transformations are the APIs which take RDD as input and return another RDD as output. These APIs does not trigger execution but update the DAG. Actions take RDD as input and return a primitive data type or regular collection to the driver program.\n",
    "\n",
    "Also, we can use actions to save the output to the files. Actions trigger execution of DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directed Acyclic Graph and Lazy Evaluation\n",
    "\n",
    "Thare are many APIs in Spark. But most of the APIs do not trigger execution of Spark job.\n",
    "\n",
    "* When we create a Spark Context object it will procure resources from the cluster\n",
    "* APIs used to read the data such as textFile as well as to process the data such as map, reduce, filter etc does not trigger immediate execution. They create variables of type RDD which also point to DAG.\n",
    "* They run in driver program and build DAG. DAG will tell how it should execute. Each variable have a DAG associated with it.\n",
    "* When APIs which are categorized as action (such as take, collect, saveAsTextFile) are used DAG associated with the variable is executed.\n",
    "* In Scala, we can look at the DAG details by using toDebugString on top of the variables created.\n",
    "* We can visualize DAG as part of Spark UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
