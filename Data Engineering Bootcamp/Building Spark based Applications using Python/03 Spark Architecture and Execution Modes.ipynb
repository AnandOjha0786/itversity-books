{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Architecture and Execution Modes\n",
    "\n",
    "As part of this topic we will understand different modules, spark architecture and how it is mapped to different execution modes such as YARN, Mesos etc.\n",
    "\n",
    "Spark is nothing but distributed computing framework. To leverage the framework we need to learn API categorized into different modules and build applications using supported programming languages (like Scala, Python, Java etc).\n",
    "\n",
    "* Spark Official Documentation\n",
    "* Spark Modules\n",
    " * Core – Transformations and Actions\n",
    " * Spark SQL and Data Frames\n",
    " * Structured Streaming\n",
    " * Machine Learning Pipelines\n",
    " * GraphX Pipelines\n",
    " * and more\n",
    "* Spark Data Structures\n",
    " * Resilient Distributed Datasets (An in memory distributed collection)\n",
    " * Data Frame (A wrapper on top of RDD with structure)\n",
    "* Spark Framework and Execution Modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Review Of APIs\n",
    "Let us have a quick review of APIs that are available in Spark.\n",
    "\n",
    "* SparkContext exposes APIs such as textFile, sequenceFile to read data from files into a distributed collection called as RDD.\n",
    "* RDD stands for Resilient Distributed Dataset and it is nothing but a distributed collection.\n",
    "* It is typically loaded on to the executors created at the time of execution.\n",
    "* RDD exposes APIs called as Transformations and Actions\n",
    "* Transformations take one RDD as input and return another RDD as output while Actions trigger execution and get data into driver program.\n",
    "* Examples of Transformations\n",
    " * Row Level Transformations – map, filter, flatMap etc\n",
    " * Aggregations – reduceByKey, aggregateByKey\n",
    " * Joins – join, leftOuterJoin, rightOuterJoin\n",
    " * Sorting – sortByKey\n",
    " * Ranking – groupByKey followed by flatMap with a lambda function\n",
    " * Except for Row Level Transformations, most of the other transformations have to go through the shuffle phase and trigger new stage.\n",
    " * Row Level Transformations are also known as Narrow Transformations.\n",
    " * Transformations that trigger shuffle and new stage are also called as Wide Transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Modules\n",
    "\n",
    "In the earlier versions of Spark we have core API at the bottom and all the higher level modules work with core API. Examples of core API are map, reduce, join, groupByKey etc. But with Spark 2, Data Frames and Spark SQL is becoming the core module.\n",
    "\n",
    "* Core – Transformations and Actions – APIs such as map, reduce, join, filter etc. They typically work on RDD\n",
    "* Spark SQL and Data Frames – APIs and Spark SQL interface for batch processing on top of Data Frames or Data Sets (not available for Python)\n",
    "* Structured Streaming – APIs and Spark SQL interface for stream data processing on top of Data Frames\n",
    "* Machine Learning Pipelines – Machine Learning data pipelines to apply Machine Learning algorithms on top of Data Frames\n",
    "* GraphX Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Data Structures\n",
    "\n",
    "We need to deal with 2 types of data structures in Spark – RDD and Data Frames.  We will see Data Structures in detail as part of the next topic.\n",
    "\n",
    "* RDD is there for quite some time and it is the low level data structure which spark uses to distribute the data between tasks while data is being processed\n",
    "* RDD will be divided into partitions while data being processed. Each partition will be processed by one task.\n",
    "* Data Frame is nothing but RDD with structure\n",
    "* Typically we read data from file systems such as HDFS, S3, Azure Blob, Local file system etc\n",
    "* Based on the file formats we need to use different APIs available in Spark to read data into RDD or Data Frame\n",
    "* Spark uses HDFS APIs to read and/or write data from underlying file system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Application\n",
    "\n",
    "Let us start with simple application to understand details related to architecture using pyspark.\n",
    "\n",
    "* As we have multiple versions of Spark on our lab and we are exploring Spark 2 we need to export SPARK_MAJOR_VERSION with 2\n",
    "* Launch pyspark using YARN and num-executors 2 (use spark.ui.port as well to specify unique port)\n",
    "* Develop simple word count program by reading data from /public/randomtextwriter/part-m-00000\n",
    "* Save output to /user/training/bootcamp/pyspark/wordcount\n",
    "\n",
    "Using this I will walk you through Spark Framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Framework\n",
    "\n",
    "Let us understand these different components of Spark Framework. Also we will understand different execution modes.\n",
    "\n",
    "* Driver Program\n",
    "* Spark Context\n",
    "* Executors\n",
    "* Executor Cache\n",
    "* Executor Tasks\n",
    "* Job\n",
    "* Stage\n",
    "* Task (Executor Tasks)\n",
    "\n",
    "Following are the different execution modes supported by Spark\n",
    "\n",
    "Local (for development)\n",
    "Standalone (for development)\n",
    "Mesos\n",
    "YARN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
