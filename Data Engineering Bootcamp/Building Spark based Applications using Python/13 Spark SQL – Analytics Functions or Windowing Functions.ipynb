{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL – Analytics Functions or Windowing Functions\n",
    "\n",
    "As part of this session we will see advanced operations such as aggregations, ranking and windowing functions with in each group using clauses such as over, partition by, order by etc. We will also build a solution for problem and run it on multinode cluster.\n",
    "\n",
    "* Aggregations, Ranking and Windowing Functions\n",
    "* Problem Statement – Get top n products per day\n",
    "* Understanding over, partition by and order by clauses\n",
    "* Performing aggregations\n",
    "* Using windowing functions\n",
    "* Ranking with in each partition or group\n",
    "* Development Life Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Life Cycle (daily product revenue)\n",
    "\n",
    "Let us develop the application using Pycharm and run it on the cluster.\n",
    "\n",
    "* Make sure application.properties have required input path and output path along with execution mode\n",
    "* Read orders and order_items data into data frames\n",
    "* Filter for complete and closed orders\n",
    "* Join with order_items\n",
    "* Aggregate to get revenue for each order_date and order_item_product_id\n",
    "* Sort in ascending order by date and then descending order by revenue\n",
    "* Save the output as CSV format\n",
    "* Validate using Pycharm\n",
    "* Ship it to the cluster, run it on the cluster and validate."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pyspark-sparksql-01-application.properties \n",
    "\n",
    "[dev]\n",
    "executionMode = local\n",
    "input.base.dir = /Users/itversity/Research/data/retail_db\n",
    "output.base.dir = /Users/itversity/Research/data/bootcamp/pyspark\n",
    "\n",
    "[prod]\n",
    "executionMode = yarn-client\n",
    "input.base.dir = /public/retail_db\n",
    "output.base.dir = /user/training/bootcamp/pyspark"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pyspark-sparksql-02-daily-product-revenue.py\n",
    "\n",
    "import configparser as cp, sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "props = cp.RawConfigParser()\n",
    "props.read('src/main/resources/application.properties')\n",
    "env = sys.argv[1]\n",
    "\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    appName(\"Daily Product Revenue using Data Frames and Spark SQL\").\\\n",
    "    master(props.get(env, 'executionMode')).\\\n",
    "    getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "inputBaseDir = props.get(env, 'input.base.dir')\n",
    "ordersCSV = spark.read. \\\n",
    "  csv(inputBaseDir + '/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "orderItemsCSV = spark.read. \\\n",
    "  csv(inputBaseDir + '/order_items'). \\\n",
    "  toDF('order_item_id', 'order_item_order_id', 'order_item_product_id',\n",
    "       'order_item_quantity', 'order_item_subtotal', 'order_item_product_price')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "orderItems = orderItemsCSV.\\\n",
    "    withColumn('order_item_id', orderItemsCSV.order_item_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_order_id', orderItemsCSV.order_item_order_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_product_id', orderItemsCSV.order_item_product_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_quantity', orderItemsCSV.order_item_quantity.cast(IntegerType())). \\\n",
    "    withColumn('order_item_subtotal', orderItemsCSV.order_item_subtotal.cast(FloatType())). \\\n",
    "    withColumn('order_item_product_price', orderItemsCSV.order_item_product_price.cast(FloatType()))\n",
    "\n",
    "orders.createTempView('orders')\n",
    "orderItems.createTempView('order_items')\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "dailyProductRevenue = spark.sql('''select o.order_date, oi.order_item_product_id, \n",
    "             round(sum(oi.order_item_subtotal), 2) as revenue\n",
    "             from orders o join order_items oi\n",
    "             on o.order_id = oi.order_item_order_id\n",
    "             where o.order_status in (\"COMPLETE\", \"CLOSED\")\n",
    "             group by o.order_date, oi.order_item_product_id\n",
    "             order by o.order_date, revenue desc''')\n",
    "\n",
    "outputBaseDir = props.get(env, 'output.base.dir')\n",
    "dailyProductRevenue.write.csv(outputBaseDir + '/daily_product_revenue_sql')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pyspark-sparksql-03-daily-product-revenue.sh \n",
    "\n",
    "spark-submit --master yarn \\\n",
    "  --deploy-mode client \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  src/main/python/retail_db/df/DailyProductRevenueDFS.py \\\n",
    "  prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations, Ranking and Windowing Functions\n",
    "\n",
    "Let us understand APIs related to aggregations, ranking and windowing functions.\n",
    "\n",
    "* There are multiple clauses with in SQL to accomplish these\n",
    " * over\n",
    " * partition by\n",
    " * order by\n",
    "* All aggregate functions, rank functions and windowing functions can be used with over clause to get aggregations per partition or group\n",
    "* It is mandatory to specify over clause\n",
    "* e.g.: rank() over(spec) where spec can be partition by or order by or both\n",
    "* Aggregations – sum, avg, min, max etc\n",
    "* Ranking – rank, dense_rank, row_number etc\n",
    "* Windowing – lead, lag etc\n",
    "* We typically use partition by clause for aggregations and then partition by as well as order by for ranking and windowing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "  builder. \\\n",
    "  master('local'). \\\n",
    "  appName('CSV Example'). \\\n",
    "  getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderItemsCSV = spark.read. \\\n",
    "  csv('/public/retail_db/order_items'). \\\n",
    "  toDF('order_item_id', 'order_item_order_id', 'order_item_product_id', \n",
    "       'order_item_quantity', 'order_item_subtotal', 'order_item_product_price')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "orderItems = orderItemsCSV.\\\n",
    "    withColumn('order_item_id', orderItemsCSV.order_item_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_order_id', orderItemsCSV.order_item_order_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_product_id', orderItemsCSV.order_item_product_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_quantity', orderItemsCSV.order_item_quantity.cast(IntegerType())). \\\n",
    "    withColumn('order_item_subtotal', orderItemsCSV.order_item_subtotal.cast(FloatType())). \\\n",
    "    withColumn('order_item_product_price', orderItemsCSV.order_item_product_price.cast(FloatType()))\n",
    "\n",
    "orderItems.createTempView('order_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-------------------+-------------+\n",
      "|order_item_id|order_item_order_id|order_item_subtotal|order_revenue|\n",
      "+-------------+-------------------+-------------------+-------------+\n",
      "|          348|                148|              100.0|       479.99|\n",
      "|          349|                148|              250.0|       479.99|\n",
      "|          350|                148|             129.99|       479.99|\n",
      "|         1129|                463|             239.96|       829.92|\n",
      "|         1130|                463|              250.0|       829.92|\n",
      "|         1131|                463|              39.99|       829.92|\n",
      "|         1132|                463|             299.97|       829.92|\n",
      "|         1153|                471|              39.99|       169.98|\n",
      "|         1154|                471|             129.99|       169.98|\n",
      "|         1223|                496|              59.99|       441.95|\n",
      "|         1224|                496|              150.0|       441.95|\n",
      "|         1225|                496|              51.99|       441.95|\n",
      "|         1226|                496|             129.99|       441.95|\n",
      "|         1227|                496|              49.98|       441.95|\n",
      "|         2703|               1088|             129.99|       249.97|\n",
      "|         2704|               1088|             119.98|       249.97|\n",
      "|         3944|               1580|             299.95|       299.95|\n",
      "|         3968|               1591|             199.95|       439.86|\n",
      "|         3969|               1591|              39.99|       439.86|\n",
      "|         3970|               1591|             199.92|       439.86|\n",
      "+-------------+-------------------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select oi.order_item_id, oi.order_item_order_id, oi.order_item_subtotal,\n",
    "             round(sum(oi.order_item_subtotal) over (partition by oi.order_item_order_id), 2) order_revenue\n",
    "             from order_items oi\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-------------------+---+\n",
      "|order_item_id|order_item_order_id|order_item_subtotal|rnk|\n",
      "+-------------+-------------------+-------------------+---+\n",
      "|          349|                148|              250.0|  1|\n",
      "|          350|                148|             129.99|  2|\n",
      "|          348|                148|              100.0|  3|\n",
      "|         1132|                463|             299.97|  1|\n",
      "|         1130|                463|              250.0|  2|\n",
      "|         1129|                463|             239.96|  3|\n",
      "|         1131|                463|              39.99|  4|\n",
      "|         1154|                471|             129.99|  1|\n",
      "|         1153|                471|              39.99|  2|\n",
      "|         1224|                496|              150.0|  1|\n",
      "|         1226|                496|             129.99|  2|\n",
      "|         1223|                496|              59.99|  3|\n",
      "|         1225|                496|              51.99|  4|\n",
      "|         1227|                496|              49.98|  5|\n",
      "|         2703|               1088|             129.99|  1|\n",
      "|         2704|               1088|             119.98|  2|\n",
      "|         3944|               1580|             299.95|  1|\n",
      "|         3968|               1591|             199.95|  1|\n",
      "|         3970|               1591|             199.92|  2|\n",
      "|         3969|               1591|              39.99|  3|\n",
      "+-------------+-------------------+-------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select oi.order_item_id, oi.order_item_order_id, oi.order_item_subtotal, \n",
    "             rank() over \n",
    "                (partition by oi.order_item_order_id \n",
    "                 order by oi.order_item_subtotal desc\n",
    "                ) rnk\n",
    "             from order_items oi\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_subtotal|next_order_item_subtotal|\n",
      "+-------------+-------------------+-------------------+------------------------+\n",
      "|          349|                148|              250.0|                  129.99|\n",
      "|          350|                148|             129.99|                   100.0|\n",
      "|          348|                148|              100.0|                    null|\n",
      "|         1132|                463|             299.97|                   250.0|\n",
      "|         1130|                463|              250.0|                  239.96|\n",
      "|         1129|                463|             239.96|                   39.99|\n",
      "|         1131|                463|              39.99|                    null|\n",
      "|         1154|                471|             129.99|                   39.99|\n",
      "|         1153|                471|              39.99|                    null|\n",
      "|         1224|                496|              150.0|                  129.99|\n",
      "|         1226|                496|             129.99|                   59.99|\n",
      "|         1223|                496|              59.99|                   51.99|\n",
      "|         1225|                496|              51.99|                   49.98|\n",
      "|         1227|                496|              49.98|                    null|\n",
      "|         2703|               1088|             129.99|                  119.98|\n",
      "|         2704|               1088|             119.98|                    null|\n",
      "|         3944|               1580|             299.95|                    null|\n",
      "|         3968|               1591|             199.95|                  199.92|\n",
      "|         3970|               1591|             199.92|                   39.99|\n",
      "|         3969|               1591|              39.99|                    null|\n",
      "+-------------+-------------------+-------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''select oi.order_item_id, oi.order_item_order_id, oi.order_item_subtotal,\n",
    "             lead(oi.order_item_subtotal) \n",
    "                  over (partition by oi.order_item_order_id \n",
    "                  order by oi.order_item_subtotal desc\n",
    "                 ) next_order_item_subtotal\n",
    "             from order_items oi\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement – Get top n products per day\n",
    "\n",
    "Let us define the problem statement and see the real usage of analytics function.\n",
    "\n",
    "* Problem Statement – Get top N Products Per day\n",
    "* Get daily product revenue code from previous topic\n",
    "* Use ranking functions and get the rank associated based on revenue for each day\n",
    "* Once we get rank, let us filter for top n products.\n",
    "\n",
    "### Understanding over, partition by and order by clauses\n",
    "\n",
    "Let us understand different clauses required for analytics functions.\n",
    "\n",
    "* Typical syntax – function(argument) over (partition by groupcolumn [order by [desc] ordercolumn])\n",
    "* For aggregations we can define group by using partition by\n",
    "* For ranking or windowing we need to use partition by and then order by. partition by is to group the data and order by is to sort the data to assign rank.\n",
    "* We will not be able to use these any where except for select clause\n",
    "* If we have to filter on these derived fields in select clause, we need to nest the whole query into another query.\n",
    "\n",
    "# Performing aggregations\n",
    "\n",
    "Let us see how to perform aggregations with in each group.\n",
    "\n",
    "* We have functions such as sum, avg, min, max etc which can be used to aggregate the data.\n",
    "* We need to use over (partition by) to get aggregations with in each group.\n",
    "* Some realistic use cases\n",
    " * Get average salary for each department and get all employee details who earn more than average salary\n",
    " * Get average revenue for each day and get all the orders who earn revenue more than average revenue\n",
    " * Get highest order revenue and get all the orders which have revenue more than 75% of the revenue\n",
    " \n",
    "### Using windowing functions\n",
    "\n",
    "Let us see details about windowing functions with in each group\n",
    "\n",
    "* We have functions such as lead, lag etc\n",
    "* We need to use partition by and then order by for most of the windowing functions\n",
    "* Some realistic use cases\n",
    " * Salary difference between current and next/previous employee with in each department\n",
    " \n",
    "### Ranking with in each partition or group\n",
    "\n",
    "Let us talk about ranking functions with in each group.\n",
    "\n",
    "* We have functions like rank, dense_rank, row_number, first, last etc\n",
    "* We need to use partition by and then order by for most of the windowing functions\n",
    "* Some realistic use cases\n",
    " * Assign rank to employees based on salary with in each department\n",
    " * Assign ranks to products based on revenue each day or month\n",
    "  \n",
    "### Development Life Cycle\n",
    "\n",
    "Let us talk about development life cycle.\n",
    "\n",
    "* Take the DailyProductRevenue code which gives us order_date, order_item_product_id and revenue\n",
    "* Add logic using function rank over partition by date and order by revenue in descending order. Make sure to give alias to this new field.\n",
    "* Nest the query into another query – e. g.: select required_fields from (query) query_alias\n",
    "* Add where clause on the query_alias.derived field name\n",
    "* Let us select and assign data related to order_date, order_item_product_id and revenue (after filtering on topN) to a new data frame\n",
    "* Save the data frame into file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pyspark-sparksql-01-application.properties\n",
    "\n",
    "[dev]\n",
    "executionMode = local\n",
    "input.base.dir = /Users/itversity/Research/data/retail_db\n",
    "output.base.dir = /Users/itversity/Research/data/bootcamp/pyspark\n",
    "\n",
    "[prod]\n",
    "executionMode = yarn-client\n",
    "input.base.dir = /public/retail_db\n",
    "output.base.dir = /user/training/bootcamp/pyspark"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pyspark-sparksql-02-topN-daily-products.py\n",
    "\n",
    "import configparser as cp, sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "props = cp.RawConfigParser()\n",
    "props.read('src/main/resources/application.properties')\n",
    "env = sys.argv[1]\n",
    "\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    appName(\"Daily Product Revenue using Data Frames and Spark SQL\").\\\n",
    "    master(props.get(env, 'executionMode')).\\\n",
    "    getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "inputBaseDir = props.get(env, 'input.base.dir')\n",
    "ordersCSV = spark.read. \\\n",
    "  csv(inputBaseDir + '/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "orderItemsCSV = spark.read. \\\n",
    "  csv(inputBaseDir + '/order_items'). \\\n",
    "  toDF('order_item_id', 'order_item_order_id', 'order_item_product_id',\n",
    "       'order_item_quantity', 'order_item_subtotal', 'order_item_product_price')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "orderItems = orderItemsCSV.\\\n",
    "    withColumn('order_item_id', orderItemsCSV.order_item_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_order_id', orderItemsCSV.order_item_order_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_product_id', orderItemsCSV.order_item_product_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_quantity', orderItemsCSV.order_item_quantity.cast(IntegerType())). \\\n",
    "    withColumn('order_item_subtotal', orderItemsCSV.order_item_subtotal.cast(FloatType())). \\\n",
    "    withColumn('order_item_product_price', orderItemsCSV.order_item_product_price.cast(FloatType()))\n",
    "\n",
    "orders.createTempView('orders')\n",
    "orderItems.createTempView('order_items')\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "dailyProductRevenue = spark.sql('''select o.order_date, oi.order_item_product_id, \n",
    "             round(sum(oi.order_item_subtotal), 2) as revenue\n",
    "             from orders o join order_items oi\n",
    "             on o.order_id = oi.order_item_order_id\n",
    "             where o.order_status in (\"COMPLETE\", \"CLOSED\")\n",
    "             group by o.order_date, oi.order_item_product_id\n",
    "             order by o.order_date, revenue desc''')\n",
    "\n",
    "dailyProductRevenue.createTempView('daily_product_revenue')\n",
    "\n",
    "topNDailyProducts = spark.sql('''select q.order_date, q.order_item_product_id, q.revenue\n",
    "             from (select order_date, order_item_product_id, revenue,\n",
    "                   rank() over (partition by order_date order by revenue desc) rnk\n",
    "                   from daily_product_revenue) q\n",
    "             where q.rnk <= %s\n",
    "             order by q.order_date, q.revenue desc''' % topN)\n",
    "\n",
    "outputBaseDir = props.get(env, 'output.base.dir')\n",
    "topNDailyProducts.write.csv(outputBaseDir + '/topn_daily_products_dfs')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pyspark-sparksql-03-topN-daily-products.sh \n",
    "\n",
    "spark-submit --master yarn \\\n",
    "  --deploy-mode client \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  src/main/python/retail_db/df/TopNDailyProductsDFS.py \\\n",
    "  prod 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
