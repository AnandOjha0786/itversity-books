{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frame Operations – Analytics Functions or Windowing Functions\n",
    "\n",
    "As part of this session we will see advanced operations such as aggregations, ranking and windowing functions with in each group using APIs such as over, partitionBy etc. We will also build a solution for problem and run it on multinode cluster.\n",
    "\n",
    "* Aggregations, Ranking and Windowing Functions – APIs\n",
    "* Problem Statement – Get top n products per day\n",
    "* Creating Window Spec\n",
    "* Performing aggregations\n",
    "* Using windowing functions\n",
    "* Ranking with in each partition or group\n",
    "* Development Life Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations, Ranking and Windowing Functions – APIs\n",
    "\n",
    "Let us understand APIs related to aggregations, ranking and windowing functions.\n",
    "\n",
    "* Main package pyspark.sql.window\n",
    "* It has classes such as Window and WindowSpec\n",
    "* Window have APIs such as partitionBy, orderBy etc\n",
    "* It return WindowSpec object. We can pass WindowSpec object to over on functions such as rank(), dense_rank(), sum() etc\n",
    "* e.g.: rank().over(spec) where spec = Window.partitionBy(‘ColumnName’)\n",
    "* Aggregations – sum, avg, min, max etc\n",
    "* Ranking – rank, dense_rank, row_number etc\n",
    "* Windowing – lead, lag etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    master('local'). \\\n",
    "    getOrCreate()\n",
    "\n",
    "orderItemsCSV = spark.read. \\\n",
    "  csv('/public/retail_db/order_items'). \\\n",
    "  toDF('order_item_id', 'order_item_order_id', 'order_item_product_id', \n",
    "       'order_item_quantity', 'order_item_subtotal', 'order_item_product_price')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "orderItems = orderItemsCSV.\\\n",
    "    withColumn('order_item_id', orderItemsCSV.order_item_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_order_id', orderItemsCSV.order_item_order_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_product_id', orderItemsCSV.order_item_product_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_quantity', orderItemsCSV.order_item_quantity.cast(IntegerType())). \\\n",
    "    withColumn('order_item_subtotal', orderItemsCSV.order_item_subtotal.cast(FloatType())). \\\n",
    "    withColumn('order_item_product_price', orderItemsCSV.order_item_product_price.cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+-------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|order_revenue|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+-------------+\n",
      "|          348|                148|                  502|                  2|              100.0|                    50.0|       479.99|\n",
      "|          349|                148|                  502|                  5|              250.0|                    50.0|       479.99|\n",
      "|          350|                148|                  403|                  1|             129.99|                  129.99|       479.99|\n",
      "|         1129|                463|                  365|                  4|             239.96|                   59.99|       829.92|\n",
      "|         1130|                463|                  502|                  5|              250.0|                    50.0|       829.92|\n",
      "|         1131|                463|                  627|                  1|              39.99|                   39.99|       829.92|\n",
      "|         1132|                463|                  191|                  3|             299.97|                   99.99|       829.92|\n",
      "|         1153|                471|                  627|                  1|              39.99|                   39.99|       169.98|\n",
      "|         1154|                471|                  403|                  1|             129.99|                  129.99|       169.98|\n",
      "|         1223|                496|                  365|                  1|              59.99|                   59.99|       441.95|\n",
      "|         1224|                496|                  502|                  3|              150.0|                    50.0|       441.95|\n",
      "|         1225|                496|                  821|                  1|              51.99|                   51.99|       441.95|\n",
      "|         1226|                496|                  403|                  1|             129.99|                  129.99|       441.95|\n",
      "|         1227|                496|                 1014|                  1|              49.98|                   49.98|       441.95|\n",
      "|         2703|               1088|                  403|                  1|             129.99|                  129.99|       249.97|\n",
      "|         2704|               1088|                  365|                  2|             119.98|                   59.99|       249.97|\n",
      "|         3944|               1580|                   44|                  5|             299.95|                   59.99|       299.95|\n",
      "|         3968|               1591|                  627|                  5|             199.95|                   39.99|       439.86|\n",
      "|         3969|               1591|                  627|                  1|              39.99|                   39.99|       439.86|\n",
      "|         3970|               1591|                 1014|                  4|             199.92|                   49.98|       439.86|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "spec = Window.partitionBy('order_item_order_id')\n",
    "\n",
    "from pyspark.sql.functions import sum, round\n",
    "orderItems. \\\n",
    "  withColumn('order_revenue', round(sum('order_item_subtotal').over(spec), 2)). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+---+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|rnk|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+---+\n",
      "|            1|                  1|                  957|                  1|             299.98|                  299.98|  1|\n",
      "|            3|                  2|                  502|                  5|              250.0|                    50.0|  1|\n",
      "|            2|                  2|                 1073|                  1|             199.99|                  199.99|  2|\n",
      "|            4|                  2|                  403|                  1|             129.99|                  129.99|  3|\n",
      "|            6|                  4|                  365|                  5|             299.95|                   59.99|  1|\n",
      "|            8|                  4|                 1014|                  4|             199.92|                   49.98|  2|\n",
      "|            7|                  4|                  502|                  3|              150.0|                    50.0|  3|\n",
      "|            5|                  4|                  897|                  2|              49.98|                   24.99|  4|\n",
      "|           12|                  5|                  957|                  1|             299.98|                  299.98|  1|\n",
      "|            9|                  5|                  957|                  1|             299.98|                  299.98|  1|\n",
      "|           10|                  5|                  365|                  5|             299.95|                   59.99|  3|\n",
      "|           13|                  5|                  403|                  1|             129.99|                  129.99|  4|\n",
      "|           11|                  5|                 1014|                  2|              99.96|                   49.98|  5|\n",
      "|           15|                  7|                  957|                  1|             299.98|                  299.98|  1|\n",
      "|           14|                  7|                 1073|                  1|             199.99|                  199.99|  2|\n",
      "|           16|                  7|                  926|                  5|              79.95|                   15.99|  3|\n",
      "|           18|                  8|                  365|                  5|             299.95|                   59.99|  1|\n",
      "|           19|                  8|                 1014|                  4|             199.92|                   49.98|  2|\n",
      "|           17|                  8|                  365|                  3|             179.97|                   59.99|  3|\n",
      "|           20|                  8|                  502|                  1|               50.0|                    50.0|  4|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "spec = Window. \\\n",
    "  partitionBy('order_item_order_id'). \\\n",
    "  orderBy(orderItems.order_item_subtotal.desc())\n",
    "\n",
    "from pyspark.sql.functions import rank\n",
    "orderItems. \\\n",
    "  withColumn('rnk', rank().over(spec)). \\\n",
    "  orderBy(orderItems.order_item_order_id, orderItems.order_item_subtotal.desc()). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|next_order_item_subtotal|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+------------------------+\n",
      "|            1|                  1|                  957|                  1|             299.98|                  299.98|                    null|\n",
      "|            3|                  2|                  502|                  5|              250.0|                    50.0|                  199.99|\n",
      "|            2|                  2|                 1073|                  1|             199.99|                  199.99|                  129.99|\n",
      "|            4|                  2|                  403|                  1|             129.99|                  129.99|                    null|\n",
      "|            6|                  4|                  365|                  5|             299.95|                   59.99|                  199.92|\n",
      "|            8|                  4|                 1014|                  4|             199.92|                   49.98|                   150.0|\n",
      "|            7|                  4|                  502|                  3|              150.0|                    50.0|                   49.98|\n",
      "|            5|                  4|                  897|                  2|              49.98|                   24.99|                    null|\n",
      "|           12|                  5|                  957|                  1|             299.98|                  299.98|                  299.95|\n",
      "|            9|                  5|                  957|                  1|             299.98|                  299.98|                  299.98|\n",
      "|           10|                  5|                  365|                  5|             299.95|                   59.99|                  129.99|\n",
      "|           13|                  5|                  403|                  1|             129.99|                  129.99|                   99.96|\n",
      "|           11|                  5|                 1014|                  2|              99.96|                   49.98|                    null|\n",
      "|           15|                  7|                  957|                  1|             299.98|                  299.98|                  199.99|\n",
      "|           14|                  7|                 1073|                  1|             199.99|                  199.99|                   79.95|\n",
      "|           16|                  7|                  926|                  5|              79.95|                   15.99|                    null|\n",
      "|           18|                  8|                  365|                  5|             299.95|                   59.99|                  199.92|\n",
      "|           19|                  8|                 1014|                  4|             199.92|                   49.98|                  179.97|\n",
      "|           17|                  8|                  365|                  3|             179.97|                   59.99|                    50.0|\n",
      "|           20|                  8|                  502|                  1|               50.0|                    50.0|                    null|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "spec = Window. \\\n",
    "  partitionBy('order_item_order_id'). \\\n",
    "  orderBy(orderItems.order_item_subtotal.desc())\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "from pyspark.sql.functions import lead\n",
    "orderItems. \\\n",
    "  withColumn('next_order_item_subtotal', lead('order_item_subtotal').over(spec)). \\\n",
    "  orderBy(orderItems.order_item_order_id, orderItems.order_item_subtotal.desc()). \\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement – Get top n products per day\n",
    "\n",
    "Let us define the problem statement and see the real usage of analytics function.\n",
    "\n",
    "* Problem Statement – Get top N Products Per day\n",
    "* Get daily product revenue code from previous topic\n",
    "* Use ranking functions and get the rank associated based on revenue for each day\n",
    "* Once we get rank, let us filter for top n products.\n",
    "\n",
    "### Creating Window Spec\n",
    "\n",
    "Let us see how to create Window Spec.\n",
    "\n",
    "* Window have APIs such as partitionBy, orderBy\n",
    "* For aggregations we can define group by using partitionBy\n",
    "* For ranking or windowing we need to use partitionBy and then orderBy. partitionBy is to group the data and orderBy is to sort the data to assign rank.\n",
    "* partitionBy or orderBy returns WindowSpec object\n",
    "* WindowSpec object need to be passed to over with ranking and aggregate functions.\n",
    "\n",
    "### Performing aggregations\n",
    "\n",
    "Let us see how to perform aggregations with in each group.\n",
    "\n",
    "* We have functions such as sum, avg, min, max etc which can be used to aggregate the data.\n",
    "* We need to create WindowSpec object using partitionBy to get aggregations with in each group.\n",
    "* Some realistic use cases\n",
    " * Get average salary for each department and get all employee details who earn more than average salary\n",
    " * Get average revenue for each day and get all the orders who earn revenue more than average revenue\n",
    " * Get highest order revenue and get all the orders which have revenue more than 75% of the revenue\n",
    " \n",
    "### Using windowing functions\n",
    "\n",
    "Let us see details about windowing functions with in each group\n",
    "\n",
    "* We have functions such as lead, lag etc\n",
    "* We need to create WindowSpec object using partitionBy and then orderBy for most of the windowing functions\n",
    "* Some realistic use cases\n",
    " * Salary difference between current and next/previous employee with in each department\n",
    "\n",
    "# Ranking with in each partition or group\n",
    "\n",
    "Let us talk about ranking functions with in each group.\n",
    "\n",
    "* We have functions like rank, dense_rank, row_number, first, last etc\n",
    "* We need to create WindowSpec object using partitionBy and then orderBy for most of the ranking functions\n",
    "* Some realistic use cases\n",
    " * Assign rank to employees based on salary with in each department\n",
    " * Assign ranks to products based on revenue each day or month\n",
    "\n",
    "### Development Life Cycle\n",
    "\n",
    "Let us talk about development life cycle.\n",
    "\n",
    "* Take the DailyProductRevenue code which gives us order_date, order_item_product_id and revenue\n",
    "* Import Window and create spec to partition by date and order by revenue in descending order.\n",
    "* Use withColumn and assign rank\n",
    "* Filter data where rank is less than or equal to topN passed as argument to the program\n",
    "* Drop rank field as we do not want to save the data and then sort in ascending order by date and descending order by revenue\n",
    "* Save the data frame into file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pyspark-dataframes-01-application.properties\n",
    "\n",
    "[dev]\n",
    "executionMode = local\n",
    "input.base.dir = /Users/itversity/Research/data/retail_db\n",
    "output.base.dir = /Users/itversity/Research/data/bootcamp/pyspark\n",
    "\n",
    "[prod]\n",
    "executionMode = yarn-client\n",
    "input.base.dir = /public/retail_db\n",
    "output.base.dir = /user/training/bootcamp/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyspark-dataframes-02-topN-daily-products.py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import configparser as cp, sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "props = cp.RawConfigParser()\n",
    "props.read('src/main/resources/application.properties')\n",
    "env = sys.argv[1]\n",
    "topN = int(sys.argv[2])\n",
    "\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    appName(\"Daily Product Revenue using Data Frame Operations\").\\\n",
    "    master(props.get(env, 'executionMode')).\\\n",
    "    getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "inputBaseDir = props.get(env, 'input.base.dir')\n",
    "ordersCSV = spark.read. \\\n",
    "  csv(inputBaseDir + '/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "orderItemsCSV = spark.read. \\\n",
    "  csv(inputBaseDir + '/order_items'). \\\n",
    "  toDF('order_item_id', 'order_item_order_id', 'order_item_product_id',\n",
    "       'order_item_quantity', 'order_item_subtotal', 'order_item_product_price')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "\n",
    "orderItems = orderItemsCSV.\\\n",
    "    withColumn('order_item_id', orderItemsCSV.order_item_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_order_id', orderItemsCSV.order_item_order_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_product_id', orderItemsCSV.order_item_product_id.cast(IntegerType())). \\\n",
    "    withColumn('order_item_quantity', orderItemsCSV.order_item_quantity.cast(IntegerType())). \\\n",
    "    withColumn('order_item_subtotal', orderItemsCSV.order_item_subtotal.cast(FloatType())). \\\n",
    "    withColumn('order_item_product_price', orderItemsCSV.order_item_product_price.cast(FloatType()))\n",
    "\n",
    "from pyspark.sql.functions import sum, round\n",
    "dailyProductRevenue = orders. \\\n",
    "    where('order_status in (\"COMPLETE\", \"CLOSED\")'). \\\n",
    "    join(orderItems, orders.order_id == orderItems.order_item_order_id). \\\n",
    "    groupBy('order_date', 'order_item_product_id'). \\\n",
    "    agg(round(sum(orderItems.order_item_subtotal), 2).alias('revenue'))\n",
    "\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "spec = Window. \\\n",
    "    partitionBy('order_date'). \\\n",
    "    orderBy(dailyProductRevenue.revenue.desc())\n",
    "\n",
    "from pyspark.sql.functions import dense_rank\n",
    "dailyProductRevenueRanked = dailyProductRevenue. \\\n",
    "    withColumn('rnk', dense_rank().over(spec))\n",
    "\n",
    "topNDailyProducts = dailyProductRevenueRanked. \\\n",
    "    where(dailyProductRevenueRanked.rnk <= topN). \\\n",
    "    drop('rnk'). \\\n",
    "    orderBy(dailyProductRevenue.order_date, dailyProductRevenue.revenue.desc())\n",
    "\n",
    "outputBaseDir = props.get(env, 'output.base.dir')\n",
    "topNDailyProducts.write.csv(outputBaseDir + '/topn_daily_products')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pyspark-dataframes-03-topN-daily-products.sh"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spark-submit --master yarn \\\n",
    "  --deploy-mode client \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  src/main/python/retail_db/df/TopNDailyProducts.py \\\n",
    "  prod 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
