{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression Concepts and Algorithms\n",
    "\n",
    "As part of this topic we will understand compression algorithms and how we can actually compress data while saving output in particular file format.\n",
    "\n",
    "* Compression Algorithms\n",
    "* Reading Compressed Data\n",
    "* Compressing while saving output\n",
    "* Criteria for choosing compression algorithm\n",
    "\n",
    "### Compression Algorithms\n",
    "Let us understand details with respect to compression algorithms.\n",
    "\n",
    "* Standard Algorithms – gzip, snappy, lzo, bzip2 etc\n",
    "* Some of the compression algorithms are splittable while others are not.\n",
    "* Most of the algorithms have both native implementation as well as java implementations (except bzip2 – which have only Java implementation)\n",
    "* Native implementations are relatively faster than Java implementations\n",
    "* Splittable vs. Non Splittable\n",
    "* We can not only compress final output, but also intermediate data in Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/08/SplittableVsNonSplittableHDFS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/08/SplittableVsNonSplittableMapReduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/08/CompressionTypesOverview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compression – Reading and Writing\n",
    "\n",
    "* Compressing text files\n",
    " * Reading – No special action need to be taken as long as we use supported algorithms.\n",
    " * Writing\n",
    "  * Can compress to most of the algorithms (bzip2, deflate, uncompressed, lz4, gzip, snappy, none)\n",
    "  * Use option on spark.write before csv – <mark>df.write.option(\"codec\", \"gzip\").csv(\"<PATH>\")</mark>\n",
    "  *  Also option with compression work fine\n",
    "* Compressing json files\n",
    " * Reading – No special action need to be taken as long as we use supported algorithms.\n",
    " * Writing\n",
    "  * Can compress to most of the algorithms (bzip2, deflate, uncompressed, lz4, gzip, snappy, none)\n",
    "  * Use option with compression – <mark>option(\"compression\", \"gzip\")</mark>\n",
    "* Compressing orc files\n",
    " * Reading – No special action need to be taken as long as we use supported algorithms.\n",
    " * Writing\n",
    "  * Default – snappy\n",
    "  * Could not figure out how I can write in other file formats\n",
    "* Compressing parquet files\n",
    " * Reading – No special action need to be taken as long as we use supported algorithms.\n",
    " * Writing\n",
    "  * Default – snappy\n",
    "  * Supported codecs – uncompressed, snappy, gzip, lzo\n",
    "  * Set spark.sql.parquet.compression.codec to the appropriate algorithm\n",
    "* Compressing avrò files\n",
    " * Reading – No special action need to be taken as long as we use supported algorithms.\n",
    " * Writing\n",
    "  * Default – uncompress\n",
    "  * Supported codecs – uncompressed, snappy, deflate\n",
    "  * Set spark.sql.avro.compression.codec to the appropriate algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession. \\\n",
    "  builder. \\\n",
    "  master('local'). \\\n",
    "  appName('CSV Example'). \\\n",
    "  getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "\n",
    "orders.write.option(\"codec\", \"gzip\"). \\\n",
    "  csv(\"/user/training/bootcampdemo/pyspark/orders_csv_compressed\")\n",
    "\n",
    "orders.write. \\\n",
    "  format('csv'). \\\n",
    "  option(\"codec\", \"gzip\"). \\\n",
    "  save(\"/user/training/bootcampdemo/pyspark/orders_csv_compressed\", mode='overwrite')\n",
    "\n",
    "orders.selectExpr(\"concat(order_id, ',', order_date, ',', order_customer_id, ',', order_status)\"). \\\n",
    "  write.option(\"codec\", \"gzip\"). \\\n",
    "  text(\"/user/training/bootcampdemo/pyspark/orders_text_compressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "  \n",
    "orders.write.option(\"codec\", \"gzip\"). \\\n",
    "  json(\"/user/training/bootcampdemo/pyspark/orders_json_compressed\")\n",
    "\n",
    "orders.write. \\\n",
    "  format('json'). \\\n",
    "  option(\"codec\", \"gzip\"). \\\n",
    "  save(\"/user/training/bootcampdemo/pyspark/orders_json_compressed\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "  \n",
    "spark.conf.set('spark.sql.parquet.compression.codec', 'gzip')\n",
    "\n",
    "orders.write. \\\n",
    "  format('parquet'). \\\n",
    "  save('/user/training/bootcampdemo/pyspark/orders_parquet_compressed')\n",
    "\n",
    "orders.write.parquet('/user/training/bootcampdemo/pyspark/orders_parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersCSV = spark.read.csv('/public/retail_db/orders'). \\\n",
    "  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')\n",
    "\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "orders = ordersCSV. \\\n",
    "  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \\\n",
    "  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))\n",
    "  \n",
    "spark.conf.set('spark.sql.avro.compression.codec', 'snappy')\n",
    "\n",
    "orders.write. \\\n",
    "  format('com.databricks.spark.avro'). \\\n",
    "  save('/user/training/bootcampdemo/pyspark/orders_avro_compressed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Criteria and Tips\n",
    "\n",
    "Here is the criteria and tips for choosing the compression algorithms.\n",
    "\n",
    "* Choose the ones with native implementation\n",
    "* Most of those compression algorithms which have native implementations are not splittable (which means irrespective of the size of the file, each file is processed by one task at a time).\n",
    "* To work around the limitation of one task per file in case of non splittable algorithms we need to make * sure data is saved in multiple files of manageable size.\n",
    "* Some of the file formats such as parquet, orc etc are compressed by default. It is better to use the default compression (for example parquet is compressed using snappy)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
